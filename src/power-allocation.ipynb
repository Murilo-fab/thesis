{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3505168d",
   "metadata": {},
   "source": [
    "1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c4337be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import warnings\n",
    "\n",
    "# Third-party imports\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Local application/library specific imports\n",
    "import DeepMIMOv3\n",
    "from LWM_v1_1 import lwm_model\n",
    "from scenario_props import *\n",
    "from generate_data import patch_maker, tokenizer\n",
    "from inference import lwm_inference\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89376e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data for 3 scenario-BS pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data Generation:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 157412.33it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:02<00:00, 17373.04it/s]\n",
      "Data Generation:  33%|███▎      | 1/3 [00:05<00:10,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 2\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 197432.40it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:02<00:00, 17994.24it/s]\n",
      "Data Generation:  67%|██████▋   | 2/3 [00:09<00:04,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 3\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 178360.42it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:02<00:00, 15708.61it/s]\n",
      "Data Generation: 100%|██████████| 3/3 [00:13<00:00,  4.53s/it]\n",
      "Sampling: 100%|██████████| 100/100 [00:00<00:00, 3449.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LWM model...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing data: 100%|██████████| 100/100 [00:00<00:00, 2103.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning embeddings...\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Constants ---\n",
    "DEFAULT_NUM_UE_ANTENNAS = 1\n",
    "DEFAULT_SUBCARRIER_SPACING = 30e3  # Hz\n",
    "DEFAULT_BS_ROTATION = np.array([0, 0, -135])  # (x, y, z) degrees\n",
    "DEFAULT_NUM_PATHS = 20\n",
    "DATASET_FOLDER = './scenarios'\n",
    "\n",
    "def get_parameters(scenario: str, bs_idx: int = 1) -> dict:\n",
    "    \"\"\"Constructs the parameter dictionary for DeepMIMOv3 data generation.\n",
    "\n",
    "    Args:\n",
    "        scenario: The name of the scenario (e.g., 'city_6_miami_v1').\n",
    "        bs_idx: The index of the active base station.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of parameters compatible with DeepMIMOv3.generate_data.\n",
    "    \"\"\"\n",
    "    # Retrieves scenario-specific properties (e.g., antenna counts)\n",
    "    scenario_configs = scenario_prop()\n",
    "    \n",
    "    # Start with default DeepMIMO parameters\n",
    "    parameters = DeepMIMOv3.default_params()\n",
    "\n",
    "    # --- Base Configuration ---\n",
    "    parameters['dataset_folder'] = DATASET_FOLDER\n",
    "    # Assumes scenario format is 'name_vX' and extracts the base name\n",
    "    parameters['scenario'] = scenario.split(\"_v\")[0]\n",
    "    parameters['active_BS'] = np.array([bs_idx])\n",
    "    parameters['enable_BS2BS'] = False\n",
    "    parameters['num_paths'] = DEFAULT_NUM_PATHS\n",
    "\n",
    "    # --- Scenario-Specific Configuration ---\n",
    "    n_ant_bs = scenario_configs[scenario]['n_ant_bs']\n",
    "    n_subcarriers = scenario_configs[scenario]['n_subcarriers']\n",
    "    user_rows_config = scenario_configs[scenario]['n_rows']\n",
    "\n",
    "    if isinstance(user_rows_config, int):\n",
    "        parameters['user_rows'] = np.arange(user_rows_config)\n",
    "    else: # Assumes a tuple or list [start, end]\n",
    "        parameters['user_rows'] = np.arange(user_rows_config[0], user_rows_config[1])\n",
    "\n",
    "    # --- Antenna and OFDM Configuration ---\n",
    "    parameters['bs_antenna']['shape'] = np.array([n_ant_bs, 1])  # [Horizontal, Vertical]\n",
    "    parameters['bs_antenna']['rotation'] = DEFAULT_BS_ROTATION\n",
    "    parameters['ue_antenna']['shape'] = np.array([DEFAULT_NUM_UE_ANTENNAS, 1])\n",
    "    parameters['OFDM']['subcarriers'] = n_subcarriers\n",
    "    parameters['OFDM']['selected_subcarriers'] = np.arange(n_subcarriers)\n",
    "    parameters['OFDM']['bandwidth'] = (DEFAULT_SUBCARRIER_SPACING * n_subcarriers) / 1e9  # GHz\n",
    "\n",
    "    return parameters\n",
    "\n",
    "def deepmimo_data_cleaning(deepmimo_data):\n",
    "    \"\"\"Cleans DeepMIMO data by removing users without a LoS path and scales channel coefficients.\n",
    "\n",
    "    Args:\n",
    "        deepmimo_data (dict): The raw data dictionary returned by DeepMIMOv3.generate_data.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cleaned and scaled channel data for users with a valid path.\n",
    "                    The channel coefficients are multiplied by CHANNEL_SCALING_FACTOR\n",
    "                    for numerical stability in subsequent processing (e.g., ML models).\n",
    "    \"\"\"\n",
    "    # Define a constant for the scaling factor\n",
    "    CHANNEL_SCALING_FACTOR = 1e6\n",
    "\n",
    "    # Identify users with a Line-of-Sight (LoS) path (LoS != -1 indicates a valid path)\n",
    "    valid_user_indices = np.where(deepmimo_data['user']['LoS'] != -1)[0]\n",
    "\n",
    "    # Select channel data only for valid users\n",
    "    cleaned_channels = deepmimo_data['user']['channel'][valid_user_indices]\n",
    "\n",
    "    # Scale the channel coefficients for numerical stability\n",
    "    return cleaned_channels * CHANNEL_SCALING_FACTOR\n",
    "\n",
    "def deepmimo_data_gen(scenario_names: list[str], bs_idxs: list[int] | None = None) -> list[dict]:\n",
    "    \"\"\"Generates DeepMIMO channel data for multiple scenarios and base stations.\n",
    "\n",
    "    Args:\n",
    "        scenario_names: A list of scenario name strings to generate data for.\n",
    "        bs_idxs: A list of base station indices to use for each scenario.\n",
    "                 Defaults to [1, 2, 3] if not provided.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary contains the 'scenario'\n",
    "        identifier and the corresponding 'channels' data (np.ndarray).\n",
    "    \"\"\"\n",
    "    if bs_idxs is None:\n",
    "        bs_idxs = [1, 2, 3]\n",
    "\n",
    "    deepmimo_data = []\n",
    "    \n",
    "    # Create a list of all (scenario, bs) pairs to iterate over\n",
    "    generation_tasks = [(name, idx) for name in scenario_names for idx in bs_idxs]\n",
    "\n",
    "    # Use tqdm for a user-friendly progress bar\n",
    "    print(f\"Generating data for {len(generation_tasks)} scenario-BS pairs...\")\n",
    "    for scenario_name, bs_idx in tqdm(generation_tasks, desc=\"Data Generation\"):\n",
    "        parameters = get_parameters(scenario_name, bs_idx)\n",
    "        # The [0] index selects the user data from the DeepMIMO output\n",
    "        raw_deepmimo_data = DeepMIMOv3.generate_data(parameters)[0]\n",
    "        cleaned_channels = deepmimo_data_cleaning(raw_deepmimo_data)\n",
    "        deepmimo_data.append({\"scenario\": f\"{scenario_name} - BS{bs_idx}\", \"channels\": cleaned_channels})\n",
    "\n",
    "    return deepmimo_data\n",
    "\n",
    "def sample(deepmimo_data: list[dict], N_samples: int, n_users: int) -> list[dict]:\n",
    "    \"\"\"Generates samples by randomly selecting a scenario and a subset of users' channels.\n",
    "\n",
    "    Each sample consists of channel data for 'n_users' randomly chosen users\n",
    "    from a randomly selected scenario.\n",
    "\n",
    "    Args:\n",
    "        deepmimo_data: A list of dictionaries, where each dict contains\n",
    "                       'scenario' (str) and 'channels' (np.ndarray) data.\n",
    "        N_samples: The total number of samples to generate.\n",
    "        n_users: The number of users (channels) to select for each sample.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, each representing a sample with 'scenario' and\n",
    "        'channels' (np.ndarray of shape (n_users, ...)).\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    for _ in tqdm(range(N_samples), desc=\"Sampling\"):\n",
    "        # Randomly select a scenario from the deepmimo_data list\n",
    "        scenario_idx = np.random.randint(0, len(deepmimo_data))\n",
    "        selected_scenario_data = deepmimo_data[scenario_idx]\n",
    "        \n",
    "        # Randomly select 'n_users' channel indices from the chosen scenario\n",
    "        num_available_users = selected_scenario_data[\"channels\"].shape[0] # Use shape[0] for number of users\n",
    "        ue_idxs = np.random.choice(num_available_users, n_users, replace=False) # Use np.random.choice for unique indices\n",
    "        \n",
    "        selected_channels = selected_scenario_data[\"channels\"][ue_idxs]\n",
    "\n",
    "        samples.append({\"scenario\": selected_scenario_data[\"scenario\"], \"channels\": selected_channels})\n",
    "\n",
    "    return samples\n",
    "\n",
    "def load_lwm_model(model_path: str, device: torch.device) -> nn.Module:\n",
    "    \"\"\"Loads the pre-trained LWM model and prepares it for inference\"\"\"\n",
    "    print(\"Loading LWM model...\")\n",
    "    model = lwm_model.lwm().to(device)\n",
    "    \n",
    "    state_dict = torch.load(model_path, map_location=device)\n",
    "    # Remove 'module.' prefix if the model was saved with DataParallel\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "\n",
    "    # Use DataParallel if multiple GPUs are available \n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Using {torch.cuda.device_count()} GPUs for inference.\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    print(\"Model loaded successfully.\")\n",
    "    return model\n",
    "    # --- Dataset Generatio ---\n",
    "\n",
    "SCENARIO_NAMES = [\"city_6_miami\"]\n",
    "BS_IDXS = [1, 2, 3]\n",
    "N_SAMPLES = 100\n",
    "N_USERS = 4\n",
    "\n",
    "deepmimo_data = deepmimo_data_gen(SCENARIO_NAMES, BS_IDXS)\n",
    "dataset = sample(deepmimo_data, N_SAMPLES, N_USERS)\n",
    "\n",
    "# --- Main Feature Extraction Logic ---\n",
    "# 1. Define configuration and load the model ONCE\n",
    "INFERENCE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LWM_MODEL_PATH = \"LWM_v1_1/models/model.pth\"\n",
    "lwm_model_instance = load_lwm_model(LWM_MODEL_PATH, INFERENCE_DEVICE)\n",
    "\n",
    "# 2. Prepare all data for batch inference\n",
    "all_tokens = []\n",
    "for item in tqdm(dataset, desc=\"Preparing data\"):\n",
    "    # Prepare input tokens for the LWM model\n",
    "    patches = patch_maker(item[\"channels\"], 4, 4)\n",
    "    tokens = tokenizer(patches)\n",
    "    all_tokens.append(tokens)\n",
    "\n",
    "    # Reshape channel data for the next stage (the regressor model)\n",
    "    # Original shape: (K, 1, M, S) -> (4, 1, 16, 32)\n",
    "    # Squeezed shape: (K, M, S) -> (4, 16, 32)\n",
    "    # Transposed shape: (S, K, M) -> (32, 4, 16)\n",
    "    item[\"channels\"] = item[\"channels\"].squeeze().transpose(2, 0, 1)\n",
    "\n",
    "# 3. Run inference on the entire batch at once\n",
    "print(\"Running batch inference...\")\n",
    "# Stack all tokens into a single tensor for efficient processing\n",
    "# Shape changes from a list of [N_USERS, ...] tensors to one [N_SAMPLES * N_USERS, ...] tensor\n",
    "all_tokens_tensor = torch.cat(all_tokens, dim=0)\n",
    "\n",
    "# Perform inference ONCE for all samples\n",
    "all_embeddings_tensor = lwm_inference(lwm_model_instance, all_tokens_tensor, \"cls_emb\", INFERENCE_DEVICE)\n",
    "\n",
    "# 4. Assign the generated embeddings back to the dataset\n",
    "print(\"Assigning embeddings...\")\n",
    "# Reshape embeddings to match the dataset structure: [N_SAMPLES, N_USERS, EMBED_DIM]\n",
    "all_embeddings_tensor = all_embeddings_tensor.view(N_SAMPLES, N_USERS, -1)\n",
    "\n",
    "\n",
    "# 1. Consolidate channels into a single tensor\n",
    "\n",
    "# The channel data was already reshaped and stored as numpy arrays in the previous step.\n",
    "all_channels_tensor = torch.from_numpy(np.array([d['channels'] for d in dataset])).cfloat()\n",
    "\n",
    "# 2. Define split sizes and batch size\n",
    "SPLIT = [int(0.7*N_SAMPLES), int(0.2*N_SAMPLES), int(0.1*N_SAMPLES)]\n",
    "BATCH_SIZE= 32\n",
    "\n",
    "# 3. Create the TensorDataset and split it\n",
    "base_dataset = TensorDataset(all_channels_tensor, all_embeddings_tensor)\n",
    "train_subset, val_subset, test_subset = random_split(base_dataset, SPLIT)\n",
    "\n",
    "# 4. Create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5743f36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-8\n",
    "LN2 = torch.log(torch.tensor(2.0))\n",
    "\n",
    "# -------------------------\n",
    "# Example regressor model\n",
    "# -------------------------\n",
    "class RegressorBeamformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      embeddings: (B, K, emb_dim)\n",
    "    Output:\n",
    "      f_r, f_i: real and imag parts of precoder columns\n",
    "        shapes: (B, S, K, M)\n",
    "      p: powers (B, S, K)\n",
    "    Parameterization: direction (unit-norm complex vector) + power scalars.\n",
    "    \"\"\"\n",
    "    def __init__(self, emb_dim, K, M, S, hidden=256):\n",
    "        super().__init__()\n",
    "        self.K = K\n",
    "        self.M = M\n",
    "        self.S = S\n",
    "\n",
    "        # Per-user encoder: shared MLP applied to each user's embedding.\n",
    "        # We process (B, K, emb_dim) as a sequence of K tokens.\n",
    "        self.user_encoder = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Cross-user interaction: a small transformer-like attention block (optional but helps)\n",
    "        # We'll implement a single self-attention layer over K users.\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=hidden, num_heads=4, batch_first=True)\n",
    "        self.attn_ln = nn.LayerNorm(hidden)\n",
    "\n",
    "        # Heads produce per-subcarrier outputs for each user:\n",
    "        # - direction head outputs 2*M*S values (real+imag flattened) per user embedding token\n",
    "        # - power head outputs S scalars per user token\n",
    "        self.dir_head = nn.Linear(hidden, S * M * 2)  # real+imag\n",
    "        self.pow_head = nn.Linear(hidden, S)          # raw scalars per subcarrier\n",
    "\n",
    "        # optionally: small residual MLP after attention\n",
    "        self.post = nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU())\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # embeddings: (B, K, emb_dim)\n",
    "        B, K, emb_dim = embeddings.shape\n",
    "        assert K == self.K, f\"K mismatch: {K} vs {self.K}\"\n",
    "\n",
    "        # encode per user\n",
    "        z = self.user_encoder(embeddings.view(B * K, emb_dim))  # (B*K, hidden)\n",
    "        z = z.view(B, K, -1)  # (B, K, hidden)\n",
    "\n",
    "        # cross-user attention (query=key=value=z)\n",
    "        attn_out, _ = self.attn(z, z, z, need_weights=False)  # (B,K,hidden)\n",
    "        z = self.attn_ln(z + attn_out)\n",
    "        z = self.post(z)  # (B,K,hidden)\n",
    "\n",
    "        # produce heads\n",
    "        dir_raw = self.dir_head(z)  # (B,K, S*M*2)\n",
    "        pow_raw = self.pow_head(z)  # (B,K,S)\n",
    "\n",
    "        # reshape directions: we want (B, S, K, M, 2)\n",
    "        dir_raw = dir_raw.view(B, K, self.S, self.M, 2).permute(0, 2, 1, 3, 4)  # (B,S,K,M,2)\n",
    "        pow_raw = pow_raw.permute(0, 2, 1)  # -> (B, S, K)  (from (B,K,S))\n",
    "\n",
    "        # separate real/imag\n",
    "        dir_r = dir_raw[..., 0]  # (B,S,K,M)\n",
    "        dir_i = dir_raw[..., 1]  # (B,S,K,M)\n",
    "\n",
    "        # normalize directions to unit-norm per (b,s,k)\n",
    "        # Stack real and imaginary parts to treat as a vector for norm calculation\n",
    "        u_complex = torch.complex(dir_r, dir_i)\n",
    "        norm = torch.linalg.vector_norm(u_complex, dim=-1, keepdim=True) # (B,S,K,1)\n",
    "        u = u_complex / (norm + EPS)\n",
    "\n",
    "        # positive scalars via softplus\n",
    "        alpha = F.softplus(pow_raw) + EPS  # (B,S,K)\n",
    "\n",
    "        return (u.real, u.imag), alpha  # directions unit-norm; alpha positive but unscaled\n",
    "\n",
    "# -------------------------\n",
    "# Sum-rate computation\n",
    "# -------------------------\n",
    "def compute_sumrate_from_directions(u_r, u_i, alpha, H_r, H_i, P_total, noise_var):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      u_r, u_i: (B, S, K, M) unit-norm directions (real, imag)\n",
    "      alpha: (B, S, K) positive raw scalars -> we'll scale to meet P_total\n",
    "      H_r, H_i: (B, S, K, M) channel real/imag (user k's row vector)\n",
    "      P_total: scalar or tensor shape (B,) total power per sample\n",
    "      noise_var: scalar or tensor shape (B, S, K) or scalar\n",
    "    Returns:\n",
    "      sumrate: (B,) bits per channel use (sum over S and K)\n",
    "      f_r, f_i: precoder columns real/imag shapes (B,S,K,M)\n",
    "      p: powers (B,S,K)\n",
    "    \"\"\"\n",
    "    B, S, K, M = u_r.shape\n",
    "    # Scale alpha to meet total-power constraint per sample\n",
    "    alpha_flat = alpha.reshape(B, -1)  # (B, S*K)\n",
    "    alpha_sum = alpha_flat.sum(dim=-1, keepdim=True)  # (B,1)\n",
    "    # allow P_total to be scalar or tensor\n",
    "    if isinstance(P_total, (float, int)):\n",
    "        P_total = torch.full((B, 1), float(P_total), device=alpha.device, dtype=alpha.dtype)\n",
    "    else:\n",
    "        P_total = P_total.view(B, 1)\n",
    "\n",
    "    p_flat = P_total * (alpha_flat / (alpha_sum + EPS))  # (B, S*K)\n",
    "    p = p_flat.view(B, S, K)  # (B,S,K)\n",
    "\n",
    "    # Construct complex tensors for channel H, direction u, and precoder F\n",
    "    H = torch.complex(H_r, H_i)  # (B,S,K,M)\n",
    "    u = torch.complex(u_r, u_i)  # (B,S,K,M)\n",
    "    sqrtp = torch.sqrt(p).unsqueeze(-1)  # (B,S,K,1)\n",
    "    F = sqrtp * u  # (B,S,K,M)\n",
    "\n",
    "    # Compute pairwise inner products G_kj = h_k^H * f_j for all k,j\n",
    "    # H is (B,S,K,M), F is (B,S,K,M). We want G of shape (B,S,K,K)\n",
    "    # G[b,s,k,j] = inner_product(H[b,s,k,:], F[b,s,j,:])\n",
    "    G = torch.einsum('bskm,bsjm->bskj', H.conj(), F) # (B,S,K,K)\n",
    "    power_matrix = G.abs()**2\n",
    "\n",
    "    # desired signal power: diagonal j==k\n",
    "    sig = torch.diagonal(power_matrix, dim1=-2, dim2=-1) # (B,S,K)\n",
    "    tot = power_matrix.sum(dim=-1) # (B,S,K)\n",
    "    interf = tot - sig\n",
    "\n",
    "    # noise_var: allow scalar or tensor\n",
    "    if isinstance(noise_var, (float, int)):\n",
    "        noise = float(noise_var)\n",
    "    else:\n",
    "        noise = noise_var\n",
    "\n",
    "    sinr = sig / (noise + interf + EPS)\n",
    "    rate = torch.log1p(sinr) / LN2  # bits/channel use\n",
    "    sumrate = rate.sum(dim=(1,2))   # sum over S and K -> (B,)\n",
    "    return sumrate, F.real, F.imag, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02a4d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/25 | Train Loss: -51.3718 | Val Loss: -52.7987\n",
      "Epoch 2/25 | Train Loss: -52.5839 | Val Loss: -53.1258\n",
      "Epoch 3/25 | Train Loss: -53.1126 | Val Loss: -53.3495\n",
      "Epoch 4/25 | Train Loss: -53.7383 | Val Loss: -53.7634\n",
      "Epoch 5/25 | Train Loss: -54.3815 | Val Loss: -54.2066\n",
      "Epoch 6/25 | Train Loss: -54.5436 | Val Loss: -54.7398\n",
      "Epoch 7/25 | Train Loss: -55.8725 | Val Loss: -55.2034\n",
      "Epoch 8/25 | Train Loss: -57.6768 | Val Loss: -55.6789\n",
      "Epoch 9/25 | Train Loss: -58.0199 | Val Loss: -55.9529\n",
      "Epoch 10/25 | Train Loss: -58.4181 | Val Loss: -56.5114\n",
      "Epoch 11/25 | Train Loss: -60.4479 | Val Loss: -57.4674\n",
      "Epoch 12/25 | Train Loss: -61.2471 | Val Loss: -58.2714\n",
      "Epoch 13/25 | Train Loss: -60.2542 | Val Loss: -59.2976\n",
      "Epoch 14/25 | Train Loss: -63.6758 | Val Loss: -60.7282\n",
      "Epoch 15/25 | Train Loss: -65.3671 | Val Loss: -61.5123\n",
      "Epoch 16/25 | Train Loss: -64.1171 | Val Loss: -62.7029\n",
      "Epoch 17/25 | Train Loss: -65.3228 | Val Loss: -63.6381\n",
      "Epoch 18/25 | Train Loss: -71.6271 | Val Loss: -64.9883\n",
      "Epoch 19/25 | Train Loss: -66.4812 | Val Loss: -66.1554\n",
      "Epoch 20/25 | Train Loss: -67.0749 | Val Loss: -66.3405\n",
      "Epoch 21/25 | Train Loss: -70.5144 | Val Loss: -67.6996\n",
      "Epoch 22/25 | Train Loss: -72.8349 | Val Loss: -69.9747\n",
      "Epoch 23/25 | Train Loss: -71.1910 | Val Loss: -71.0210\n",
      "Epoch 24/25 | Train Loss: -73.0666 | Val Loss: -71.4747\n",
      "Epoch 25/25 | Train Loss: -72.7167 | Val Loss: -72.4494\n"
     ]
    }
   ],
   "source": [
    "# --- Training Configuration ---\n",
    "N_EPOCHS = 25\n",
    "LEARNING_RATE = 1e-4\n",
    "P_TOTAL = 1.0\n",
    "NOISE_VARIANCE = 1e-3\n",
    "\n",
    "# --- Model and Optimizer Initialization ---\n",
    "# Extract dimensions from data\n",
    "_, S, K, M = all_channels_tensor.shape\n",
    "_, _, EMBED_DIM = all_embeddings_tensor.shape\n",
    "\n",
    "# Instantiate the regressor model and move it to the correct device\n",
    "regressor_model = RegressorBeamformer(emb_dim=EMBED_DIM, K=K, M=M, S=S).to(INFERENCE_DEVICE)\n",
    "\n",
    "# Setup the optimizer\n",
    "optimizer = torch.optim.Adam(regressor_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # --- Training Step ---\n",
    "    regressor_model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for channels_batch, embeddings_batch in train_loader:\n",
    "        # Move batch to device\n",
    "        channels_batch = channels_batch.to(INFERENCE_DEVICE)\n",
    "        embeddings_batch = embeddings_batch.to(INFERENCE_DEVICE)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass: get directions and power scalars from the model\n",
    "        (u_r, u_i), alpha = regressor_model(embeddings_batch)\n",
    "\n",
    "        # Calculate sum-rate \n",
    "        sumrate_batch, _, _, _ = compute_sumrate_from_directions(u_r, u_i, alpha,\n",
    "                                                                 channels_batch.real, channels_batch.imag,\n",
    "                                                                 P_TOTAL, NOISE_VARIANCE)\n",
    "        \n",
    "        # The loss is the negative of the sum-rate\n",
    "        loss = -torch.mean(sumrate_batch)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "    # --- Validation Step ---\n",
    "    regressor_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for channels_batch, embeddings_batch in val_loader:\n",
    "            # Move batch to device\n",
    "            channels_batch = channels_batch.to(INFERENCE_DEVICE)\n",
    "            embeddings_batch = embeddings_batch.to(INFERENCE_DEVICE)\n",
    "\n",
    "            (u_r, u_i), alpha = regressor_model(embeddings_batch)\n",
    "\n",
    "            sumrate_batch, _, _, _ = compute_sumrate_from_directions(u_r, u_i, alpha,\n",
    "                                                                 channels_batch.real, channels_batch.imag,\n",
    "                                                                 P_TOTAL, NOISE_VARIANCE)\n",
    "\n",
    "            loss = -torch.mean(sumrate_batch)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
