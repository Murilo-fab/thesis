{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec0aa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Multi-Scenario Dataset (5 cities)...\n",
      "  > Processing city_7_sandiego... \n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 5893/5893 [00:00<00:00, 59451.25it/s]\n",
      "Generating channels: 100%|██████████| 5893/5893 [00:00<00:00, 6724.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. (+2192 samples)\n",
      "  > Processing city_11_santaclara... \n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 5244/5244 [00:00<00:00, 94651.92it/s]\n",
      "Generating channels: 100%|██████████| 5244/5244 [00:00<00:00, 7257.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. (+2644 samples)\n",
      "  > Processing city_12_fortworth... \n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 6120/6120 [00:00<00:00, 122714.93it/s]\n",
      "Generating channels: 100%|██████████| 6120/6120 [00:00<00:00, 8827.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. (+1901 samples)\n",
      "  > Processing city_15_indianapolis... \n",
      "Basestation 3\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 6241/6241 [00:00<00:00, 93796.56it/s]\n",
      "Generating channels: 100%|██████████| 6241/6241 [00:00<00:00, 7448.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. (+3222 samples)\n",
      "  > Processing city_19_oklahoma... \n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 6075/6075 [00:00<00:00, 88058.68it/s]\n",
      "Generating channels: 100%|██████████| 6075/6075 [00:00<00:00, 6677.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. (+3442 samples)\n",
      "--- Total Dataset: 13401 samples. Shape: torch.Size([13401, 32, 32]) ---\n",
      "Loading Multi-Scenario Dataset (1 cities)...\n",
      "  > Processing city_6_miami... \n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 148119.64it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:03<00:00, 11793.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success. (+10441 samples)\n",
      "--- Total Dataset: 10441 samples. Shape: torch.Size([10441, 32, 32]) ---\n",
      "\n",
      "Starting Training: csi_autoencoder_pretrain_128\n",
      "Ref Power: 1.319011e+03\n",
      "Epoch 010 | Train: 1.000144 | Val: 0.999974 | Time: 397.01s\n",
      "Epoch 020 | Train: 1.000145 | Val: 1.000406 | Time: 830.86s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from thesis.data_classes import AutoEncoderConfig\n",
    "from thesis.csi_autoencoder import MultiCityGenerator, CSIAutoEncoder, train_model, plot_training_history\n",
    "\n",
    "config = AutoEncoderConfig(latent_dim=128)\n",
    "    \n",
    "# 1. Load Data\n",
    "train_gen = MultiCityGenerator(config.train_cities, config.scale_factor)\n",
    "val_gen = MultiCityGenerator(config.val_cities, config.scale_factor)\n",
    "\n",
    "X_train, _ = train_gen.load_all()\n",
    "X_val, _ = val_gen.load_all()\n",
    "\n",
    "train_dl = DataLoader(TensorDataset(X_train), batch_size=config.batch_size, shuffle=True)\n",
    "val_dl = DataLoader(TensorDataset(X_val), batch_size=config.batch_size, shuffle=False)\n",
    "\n",
    "# 2. Physics Calibration (Global Reference Power)\n",
    "# Calculate average power of the training set to set the Noise Floor baseline\n",
    "ref_power = torch.mean(torch.abs(X_train)**2).item()\n",
    "\n",
    "# 3. Model\n",
    "model = CSIAutoEncoder(latent_dim=config.latent_dim, mode=\"train\")\n",
    "\n",
    "# 4. Train\n",
    "best_model, history, res_folder = train_model(model, train_dl, val_dl, config, ref_power)\n",
    "\n",
    "# 5. Visualize\n",
    "plot_training_history(history, save_dir=res_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904f95d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
