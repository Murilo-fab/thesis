{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e54dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.dataset import PowerAllocationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54919f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 111689.71it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:06<00:00, 6910.42it/s] \n",
      "Generating Scenarios: 100%|██████████| 5000/5000 [00:07<00:00, 644.92it/s] \n"
     ]
    }
   ],
   "source": [
    "# General parameters\n",
    "SEED = 42\n",
    "INFERENCE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_USERS = [4][0]\n",
    "SCENARIO_NAME = \"city_6_miami\"\n",
    "BS_IDX = 1\n",
    "\n",
    "# Training Parameters\n",
    "EPOCHS = 15\n",
    "WARMUP_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "FINETUNE_LEARNING_RATE = 1e-5\n",
    "TRAINING_RATIOS = [1.0][0]\n",
    "\n",
    "D_MODEL = 128\n",
    "\n",
    "optimizer_configs = {\"task_head_lr\": LEARNING_RATE,\n",
    "                    \"encoder_lr\": FINETUNE_LEARNING_RATE}\n",
    "\n",
    "dataset = PowerAllocationDataset(num_samples=NUM_SAMPLES,\n",
    "                                 num_users=NUM_USERS,\n",
    "                                 scenario_name=SCENARIO_NAME,\n",
    "                                 bs_idx=BS_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4174c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./models/model.pth\n",
      "* Trackio project initialized: power_allocation\n",
      "* Trackio metrics logged to: /home/murilo/.cache/huggingface/trackio\n",
      "* View dashboard by running in your terminal:\n",
      "\u001b[1m\u001b[38;5;208mtrackio show --project \"power_allocation\"\u001b[0m\n",
      "* or by running in Python: trackio.show(project=\"power_allocation\")\n",
      "* Created new run: Run_2025-12-20_20-47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [05:13<00:00, 20.91s/it, Train Loss=-14.8, Validation Loss=-14.9, LR=0.001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Run finished. Uploading logs to Trackio (please wait...)\n"
     ]
    }
   ],
   "source": [
    "from src.utils import prepare_loaders, get_subset\n",
    "from src.lwm_model import lwm\n",
    "from src.downstream_models import RegressionHead, Wrapper\n",
    "from src.metrics import SumRateLoss, benchmark\n",
    "from src.train import train_downstream_model\n",
    "\n",
    "P_TOTAL = 20.0\n",
    "NOISE_VARIANCE = 1e-13\n",
    "results_folder = f\"./results\"\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_loaders(dataset.raw_channels, dataset.data_tokens, seed=SEED)\n",
    "fraction_train_loader = get_subset(train_loader, TRAINING_RATIOS, seed=SEED)\n",
    "\n",
    "\n",
    "lwm_model = lwm.from_pretrained(ckpt_name=\"./models/model.pth\", device=INFERENCE_DEVICE)\n",
    "task_head = RegressionHead(D_MODEL, num_subcarriers=32)\n",
    "\n",
    "model = Wrapper(lwm_model, task_head).to(INFERENCE_DEVICE)\n",
    "\n",
    "criterion = SumRateLoss(P_TOTAL, NOISE_VARIANCE)\n",
    "\n",
    "trackio_params = {\n",
    "    \"project\": \"power_allocation\",\n",
    "    \"group\": \"MRT\",\n",
    "    \"config\": {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"device\": INFERENCE_DEVICE,\n",
    "        \"p_total\": P_TOTAL,\n",
    "        \"noise_variance\": NOISE_VARIANCE\n",
    "    }\n",
    "}\n",
    "\n",
    "model= train_downstream_model(model=model,\n",
    "                              train_loader=fraction_train_loader,\n",
    "                              val_loader=val_loader,\n",
    "                              optimizer_configs=optimizer_configs,\n",
    "                              criterion=criterion,\n",
    "                              epochs=EPOCHS,\n",
    "                              device=INFERENCE_DEVICE,\n",
    "                              save_dir=results_folder,\n",
    "                              trackio_params=trackio_params)\n",
    "\n",
    "# print(benchmark(model, test_loader, P_TOTAL, NOISE_VARIANCE, INFERENCE_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "087bd9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 16/16 [00:07<00:00,  2.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "POWER CONSUMPTION REPORT\n",
      "========================================\n",
      "Target P_total: 20.0000\n",
      "Actual ML Power: 20.0000\n",
      "Actual WF Power: 20.0000\n",
      "SUCCESS: Power budgets are matched.\n",
      "========================================\n",
      "\n",
      "{'ZF+ML': np.float64(11.671521663665771), 'MRT+ML': np.float64(14.858694016933441), 'ZF+WF': np.float64(28.817013025283813), 'MRT+WF': np.float64(3.069923371076584)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(benchmark(model, test_loader, P_TOTAL, NOISE_VARIANCE, device=\"cpu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9427332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_noise_power(bandwidth_ghz, noise_figure_db=9):\n",
    "    \"\"\"\n",
    "    Calculates noise variance (sigma^2) in linear scale (Watts).\n",
    "    \"\"\"\n",
    "    k_B = 1.380649e-23  # Boltzmann constant\n",
    "    T = 290             # Temperature (Kelvin)\n",
    "    BW_Hz = bandwidth_ghz * 1e9 # Convert GHz to Hz\n",
    "    \n",
    "    # Thermal Noise Density (N0)\n",
    "    noise_spectral_density = k_B * T \n",
    "    \n",
    "    # Noise Figure in Linear Scale\n",
    "    noise_figure_linear = 10 ** (noise_figure_db / 10)\n",
    "    \n",
    "    # Total Noise Power\n",
    "    noise_power_watts = noise_spectral_density * BW_Hz * noise_figure_linear\n",
    "    \n",
    "    return noise_power_watts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b063fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RefineBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv1 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "\n",
    "        # Layer 2\n",
    "        self.bn2 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # Path\n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # Residual Connection\n",
    "        out = out + shortcut\n",
    "        return out\n",
    "\n",
    "class BeamNetDecoder(nn.Module):\n",
    "    def __init__(self, num_users, num_carriers, num_antennas, embedding_dim, filters=64, p_max=1.0):\n",
    "        super(BeamNetDecoder, self).__init__()\n",
    "        \n",
    "        self.K = num_users\n",
    "        self.Nc = num_carriers\n",
    "        self.Nt = num_antennas\n",
    "        self.filters = filters\n",
    "        self.p_max = p_max\n",
    "        \n",
    "        # 1. Projection\n",
    "        # Input: (Batch, K, Emb) -> (Batch, K, Filters * Nc)\n",
    "        self.user_projection = nn.Linear(embedding_dim, filters * num_carriers)\n",
    "        \n",
    "        # 2. RefineNet Core (Same as before)\n",
    "        self.conv_input = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.refine1 = RefineBlock(filters) # (defined in previous turn)\n",
    "        self.refine2 = RefineBlock(filters)\n",
    "        \n",
    "        # 3. Beam Head (Expansion)\n",
    "        # We need to output Real and Imag parts for every Antenna\n",
    "        # Output Channels = 2 * Nt\n",
    "        self.beam_head = nn.Conv2d(filters, 2 * num_antennas, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        B = z.size(0)\n",
    "        \n",
    "        # --- A. Expand & Reshape ---\n",
    "        x = self.user_projection(z)\n",
    "        x = x.view(B, self.K, self.filters, self.Nc)\n",
    "        x = x.permute(0, 2, 1, 3) # (Batch, Filters, Users, Carriers)\n",
    "        \n",
    "        # --- B. RefineNet (Interference Management) ---\n",
    "        x = self.conv_input(x)\n",
    "        x = self.refine1(x)\n",
    "        x = self.refine2(x)\n",
    "        \n",
    "        # --- C. Generate Raw Complex Vectors ---\n",
    "        # Shape: (Batch, 2*Nt, Users, Carriers)\n",
    "        raw_beams = self.beam_head(x)\n",
    "        \n",
    "        # Separate Real and Imaginary parts\n",
    "        # Reshape to: (Batch, Users, Carriers, Antennas, 2)\n",
    "        # 1. Permute to put channels last: (Batch, Users, Carriers, 2*Nt)\n",
    "        raw_beams = raw_beams.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # 2. View to separate Real/Imag: (Batch, Users, Carriers, Nt, 2)\n",
    "        w = raw_beams.view(B, self.K, self.Nc, self.Nt, 2)\n",
    "        \n",
    "        # --- D. Power Constraint (L2 Normalization) ---\n",
    "        # Calculate Power per beam: |w|^2 = Re^2 + Im^2\n",
    "        # Sum over Antennas (dim 3) and Re/Im (dim 4)\n",
    "        beam_power = torch.sum(w ** 2, dim=(3, 4), keepdim=True) # (B, K, Nc, 1, 1)\n",
    "        \n",
    "        # Total Power used in the system (sum over Users and Carriers)\n",
    "        total_system_power = torch.sum(beam_power, dim=(1, 2), keepdim=True)\n",
    "        \n",
    "        # Scale factor to satisfy P_max\n",
    "        # We clamp the divisor to avoid exploding gradients if power is near 0\n",
    "        scale = torch.sqrt(self.p_max / (total_system_power + 1e-8))\n",
    "        \n",
    "        # Apply scaling\n",
    "        w_final = w * scale\n",
    "        \n",
    "        w_final = w_final.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        \n",
    "        return w_final\n",
    "    \n",
    "class Wrapper(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 task_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = model\n",
    "        self.task_head = task_head\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def fine_tune(self, fine_tune_layers=\"full\"):\n",
    "        if fine_tune_layers == \"full\":\n",
    "             for param in self.encoder.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            for name, param in self.encoder.named_parameters():\n",
    "                if any(layer in name for layer in fine_tune_layers):\n",
    "                    param.requires_grad = True\n",
    "         \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward function for Wrapper\n",
    "        \n",
    "        Inputs:\n",
    "        channels (torch.tensor): Channel matrix [B, S, K, N]\n",
    "\n",
    "        Outputs:\n",
    "        power_weights (torch.tensor): Normalized weights [B, S, K]\n",
    "        \"\"\"\n",
    "        # 1. Extract channel shape\n",
    "        B, K, S, F = tokens.shape\n",
    "\n",
    "        # 2. Flatten for Encoder\n",
    "        # Shape: [B, K, S, F] : [B*K, S, F]\n",
    "        x = tokens.view(B*K, S, F)\n",
    "\n",
    "        # 3. Encoder\n",
    "        # Embeddings shape: [B*K, S, d_model]\n",
    "        embeddings, _ = self.encoder(x)\n",
    "\n",
    "        # 4. Extract CLS Token\n",
    "        # Shape: [B*K, d_model]\n",
    "        cls_embedding = embeddings[:, 0, :]\n",
    "\n",
    "        # 5. Reshape to [Batch, Users, D] BEFORE the head\n",
    "        # This is the key change. We reconstruct the user dimension here.\n",
    "        cls_structured = cls_embedding.view(B, K, -1)\n",
    "\n",
    "        # 6. Head Pass\n",
    "        # The head now takes the structured data and returns normalized power\n",
    "        W = self.task_head(cls_structured)\n",
    "\n",
    "        return W"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
