{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7e2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 144732.07it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:05<00:00, 7485.48it/s] \n",
      "Generating Scenarios: 100%|██████████| 450/450 [00:00<00:00, 554.31it/s]\n",
      "Generating Scenarios: 100%|██████████| 450/450 [00:02<00:00, 158.54it/s]\n",
      "Generating Scenarios: 100%|██████████| 100/100 [00:00<00:00, 338.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from thesis.utils import prepare_loaders, calculate_noise_variance\n",
    "from thesis.dataset import DeepMIMOGenerator\n",
    "\n",
    "datasets = []\n",
    "num_users = 8\n",
    "\n",
    "generator = DeepMIMOGenerator()\n",
    "total_samples = 1000\n",
    "\n",
    "data_spatial, _ = generator.generate_dataset(\n",
    "        num_samples=int(total_samples * 0.45),\n",
    "        num_users=num_users,\n",
    "        min_corr=0.90, \n",
    "        max_corr=0.99,\n",
    "        max_gain_ratio=10.0 \n",
    "    )\n",
    "\n",
    "data_power, _ = generator.generate_dataset(\n",
    "        num_samples=int(total_samples * 0.45),\n",
    "        num_users=num_users,\n",
    "        min_corr=0.0,\n",
    "        max_corr=0.5,\n",
    "        max_gain_ratio=20.0\n",
    "    )\n",
    "\n",
    "data_std, _ = generator.generate_dataset(\n",
    "        num_samples=int(total_samples * 0.10),\n",
    "        num_users=num_users,\n",
    "        min_corr=0.0,\n",
    "        max_corr=0.7,\n",
    "        max_gain_ratio=20.0\n",
    "    )\n",
    "\n",
    "final_dataset = np.concatenate([data_spatial, data_power, data_std], axis=0)\n",
    "dataset_tensor = torch.tensor(final_dataset)\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_loaders(dataset_tensor)\n",
    "\n",
    "noise_variance = calculate_noise_variance(dataset_tensor, 5, \"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8d378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./models/model.pth\n",
      "Initializing Joint Training...\n",
      "Epoch 1/50 | Temp: 5.00 | Train Loss: -1.9597\n",
      "Epoch 2/50 | Temp: 4.75 | Train Loss: -1.9717\n",
      "Epoch 3/50 | Temp: 4.51 | Train Loss: -1.9884\n",
      "Epoch 4/50 | Temp: 4.26 | Train Loss: -2.0043\n",
      "Epoch 5/50 | Temp: 4.02 | Train Loss: -2.0339\n",
      "   >> Validation Loss: -5.3532\n",
      "Epoch 6/50 | Temp: 3.77 | Train Loss: -2.0683\n",
      "Epoch 7/50 | Temp: 3.53 | Train Loss: -2.1084\n",
      "Epoch 8/50 | Temp: 3.29 | Train Loss: -2.1662\n",
      "Epoch 9/50 | Temp: 3.04 | Train Loss: -2.2254\n",
      "Epoch 10/50 | Temp: 2.79 | Train Loss: -2.3090\n",
      "   >> Validation Loss: -8.6664\n",
      "Epoch 11/50 | Temp: 2.55 | Train Loss: -2.4167\n",
      "Epoch 12/50 | Temp: 2.30 | Train Loss: -2.5548\n",
      "Epoch 13/50 | Temp: 2.06 | Train Loss: -2.7464\n",
      "Epoch 14/50 | Temp: 1.81 | Train Loss: -3.0064\n",
      "Epoch 15/50 | Temp: 1.57 | Train Loss: -3.3839\n",
      "   >> Validation Loss: -11.1768\n",
      "Epoch 16/50 | Temp: 1.32 | Train Loss: -3.9520\n",
      "Epoch 17/50 | Temp: 1.08 | Train Loss: -4.8173\n",
      "Epoch 18/50 | Temp: 0.83 | Train Loss: -6.1871\n",
      "Epoch 19/50 | Temp: 0.59 | Train Loss: -8.2222\n",
      "Epoch 20/50 | Temp: 0.34 | Train Loss: -10.8891\n",
      "   >> Validation Loss: -15.3656\n",
      "Epoch 21/50 | Temp: 0.10 | Train Loss: -13.8714\n",
      "Epoch 22/50 | Temp: 0.10 | Train Loss: -15.1002\n",
      "Epoch 23/50 | Temp: 0.10 | Train Loss: -16.4566\n",
      "Epoch 24/50 | Temp: 0.10 | Train Loss: -17.9075\n",
      "Epoch 25/50 | Temp: 0.10 | Train Loss: -19.5556\n",
      "   >> Validation Loss: -23.6401\n",
      "Epoch 26/50 | Temp: 0.10 | Train Loss: -21.2913\n",
      "Epoch 27/50 | Temp: 0.10 | Train Loss: -23.2383\n",
      "Epoch 28/50 | Temp: 0.10 | Train Loss: -25.5185\n",
      "Epoch 29/50 | Temp: 0.10 | Train Loss: -27.7292\n",
      "Epoch 30/50 | Temp: 0.10 | Train Loss: -30.2639\n",
      "   >> Validation Loss: -35.0693\n",
      "Epoch 31/50 | Temp: 0.10 | Train Loss: -32.6018\n",
      "Epoch 32/50 | Temp: 0.10 | Train Loss: -35.1978\n",
      "Epoch 33/50 | Temp: 0.10 | Train Loss: -37.7179\n",
      "Epoch 34/50 | Temp: 0.10 | Train Loss: -40.2912\n",
      "Epoch 35/50 | Temp: 0.10 | Train Loss: -42.5527\n",
      "   >> Validation Loss: -47.4595\n",
      "Epoch 36/50 | Temp: 0.10 | Train Loss: -44.5905\n",
      "Epoch 37/50 | Temp: 0.10 | Train Loss: -46.3984\n",
      "Epoch 38/50 | Temp: 0.10 | Train Loss: -47.6214\n",
      "Epoch 39/50 | Temp: 0.10 | Train Loss: -48.7333\n",
      "Epoch 40/50 | Temp: 0.10 | Train Loss: -49.7674\n",
      "   >> Validation Loss: -54.2200\n",
      "Epoch 41/50 | Temp: 0.10 | Train Loss: -50.4080\n",
      "Epoch 42/50 | Temp: 0.10 | Train Loss: -51.2983\n",
      "Epoch 43/50 | Temp: 0.10 | Train Loss: -51.5757\n",
      "Epoch 44/50 | Temp: 0.10 | Train Loss: -52.1383\n",
      "Epoch 45/50 | Temp: 0.10 | Train Loss: -52.5705\n",
      "   >> Validation Loss: -57.1947\n",
      "Epoch 46/50 | Temp: 0.10 | Train Loss: -53.0363\n",
      "Epoch 47/50 | Temp: 0.10 | Train Loss: -53.2778\n",
      "Epoch 48/50 | Temp: 0.10 | Train Loss: -53.7729\n",
      "Epoch 49/50 | Temp: 0.10 | Train Loss: -53.9137\n",
      "Epoch 50/50 | Temp: 0.10 | Train Loss: -54.0430\n",
      "   >> Validation Loss: -58.6464\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from thesis.user_scheduling import JointUtilityLoss\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Annealing Schedule \n",
    "START_TEMP = 5.0   # High temp = Fuzzy exploration\n",
    "END_TEMP = 0.1     # Low temp = Hard decision (Deployment mode)\n",
    "ANNEAL_EPOCHS = 20 # Reach end_temp by epoch 20\n",
    "\n",
    "def get_current_temperature(epoch):\n",
    "    \"\"\"Calculates Softmax temperature for the current epoch.\"\"\"\n",
    "    if epoch >= ANNEAL_EPOCHS:\n",
    "        return END_TEMP\n",
    "    decay = (START_TEMP - END_TEMP) / ANNEAL_EPOCHS\n",
    "    return START_TEMP - (decay * epoch)\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # Use hard/sharp temperature for validation to mimic real performance\n",
    "    val_temp = END_TEMP \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            channels_raw = batch[0].to(device)\n",
    "            \n",
    "            # Forward\n",
    "            probs, powers = model(channels_raw, temperature=val_temp)\n",
    "            \n",
    "            # Physics World: Real Metrics\n",
    "            loss = criterion(probs, powers, channels_raw)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    print(f\"   >> Validation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_joint_scheduler(model, train_loader, val_loader, noise_variance):\n",
    "    print(\"Initializing Joint Training...\")\n",
    "    \n",
    "    # Optimizer: Updates both LWM backbone (if unfrozen) and Heads\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Loss: Joint Utility\n",
    "    criterion = JointUtilityLoss(alpha_entropy=0.05, alpha_power=0.01, noise_var=noise_variance)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        current_temp = get_current_temperature(epoch)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            channels_raw = batch[0].to(DEVICE)\n",
    "            \n",
    "            # 1. Forward Pass ---\n",
    "            probs, powers = model(channels_raw, temperature=current_temp)\n",
    "            \n",
    "            # 2. Loss Calculation ---\n",
    "            loss = criterion(probs, powers, channels_raw)\n",
    "            \n",
    "            # 3. Optimization ---\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping (Crucial for LWM stability)\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # --- Logging & Validation ---\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Temp: {current_temp:.2f} | Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validate every 5 epochs or at the end\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "# --- Usage ---\n",
    "from thesis.dataset import Tokenizer\n",
    "from thesis.downstream_models import WrapperAllocation, AssignmentHead, PowerAllocationHead\n",
    "from thesis.lwm_model import lwm\n",
    "\n",
    "tokenizer = Tokenizer(4, 4)\n",
    "lwm_model = lwm.from_pretrained(\"./models/model.pth\")\n",
    "assignment_head = AssignmentHead()\n",
    "allocation_head = PowerAllocationHead()\n",
    "full_model = WrapperAllocation(tokenizer, lwm_model, assignment_head, allocation_head)\n",
    "\n",
    "train_joint_scheduler(full_model, train_loader, val_loader, noise_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f94bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Benchmark (Noise=6.1e-10)...\n",
      "\n",
      "================================================================================\n",
      "FINAL BENCHMARK RESULTS\n",
      "================================================================================\n",
      "        Algorithm  Sum Rate  Fairness  Edge Rate\n",
      "     Greedy (EPA) 63.265110  0.140501   0.000000\n",
      " AI Model (Joint) 39.401573  0.169112   0.000000\n",
      "Round Robin (EPA) 26.151699  0.537629   0.000505\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class WirelessBenchmark:\n",
    "    def __init__(self, model, patch_cols, noise_variance=1e-13, total_power=1.0, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.patch_cols = patch_cols\n",
    "        self.noise = noise_variance\n",
    "        self.total_power = total_power\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Removed \"Proportional Fair (EPA)\"\n",
    "        self.global_results = {\n",
    "            \"Greedy (EPA)\": [], \n",
    "            \"Round Robin (EPA)\": [], \n",
    "            \"AI Model (Joint)\": []\n",
    "        }\n",
    "\n",
    "    def _calculate_high_fidelity_rates(self, channels, allocation_map, power_map_per_block):\n",
    "        # ... (Same Physics Logic as before) ...\n",
    "        B, K, M, SC = channels.shape\n",
    "        Num_Blocks = allocation_map.shape[1]\n",
    "        SC_per_block = SC // Num_Blocks\n",
    "\n",
    "        # Expand to Subcarriers\n",
    "        alloc_sc = allocation_map.repeat_interleave(SC_per_block, dim=1)\n",
    "        power_sc = (power_map_per_block / SC_per_block).repeat_interleave(SC_per_block, dim=1)\n",
    "        \n",
    "        # Physics (MRC)\n",
    "        channel_gains_sc = torch.sum(torch.abs(channels)**2, dim=2)\n",
    "        winner_gains_sc = torch.gather(channel_gains_sc, 1, alloc_sc.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Rate Calculation\n",
    "        snr_sc = (power_sc * winner_gains_sc) / self.noise\n",
    "        rate_sc = torch.log2(1 + snr_sc)\n",
    "        \n",
    "        # Aggregation per User\n",
    "        user_indices = torch.arange(K, device=self.device).view(1, K, 1)\n",
    "        mask = (alloc_sc.unsqueeze(1) == user_indices).float()\n",
    "        user_rates = torch.sum(rate_sc.unsqueeze(1) * mask, dim=2) # (B, K)\n",
    "        \n",
    "        return user_rates\n",
    "\n",
    "    def run_baselines(self, channels):\n",
    "        B, K, M, SC = channels.shape\n",
    "        num_blocks = SC // self.patch_cols\n",
    "        \n",
    "        # Pre-calc block stats\n",
    "        mag_sq = torch.abs(channels)**2\n",
    "        gain_freq = torch.mean(mag_sq, dim=2)\n",
    "        gain_reshaped = gain_freq.view(B, K, num_blocks, self.patch_cols)\n",
    "        block_gains = torch.mean(gain_reshaped, dim=3)\n",
    "        \n",
    "        power_per_block = self.total_power / num_blocks\n",
    "        epa_map = torch.full((B, num_blocks), power_per_block, device=self.device)\n",
    "        \n",
    "        block_snrs = (block_gains * power_per_block) / self.noise\n",
    "        block_rates = torch.log2(1 + block_snrs)\n",
    "        \n",
    "        # 1. Greedy\n",
    "        greedy_alloc = torch.argmax(block_rates, dim=1)\n",
    "        self.global_results[\"Greedy (EPA)\"].append(\n",
    "            self._calculate_high_fidelity_rates(channels, greedy_alloc, epa_map))\n",
    "            \n",
    "        # 2. Round Robin\n",
    "        start_offsets = torch.randint(0, K, (B, 1), device=self.device)\n",
    "        rr_alloc = (torch.arange(num_blocks, device=self.device).unsqueeze(0) + start_offsets) % K\n",
    "        self.global_results[\"Round Robin (EPA)\"].append(\n",
    "            self._calculate_high_fidelity_rates(channels, rr_alloc, epa_map))\n",
    "            \n",
    "        # REMOVED: Proportional Fair Logic\n",
    "\n",
    "    def run_ai(self, channels):\n",
    "        # ... (Same AI Logic as before) ...\n",
    "        batch_max = torch.amax(torch.abs(channels), dim=(1,2,3), keepdim=True)\n",
    "        channels_norm = channels / (batch_max + 1e-12)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, powers = self.model(channels_norm, temperature=0.01)\n",
    "            ai_alloc = torch.argmax(probs, dim=1)\n",
    "            ai_power_raw = torch.gather(powers, 1, ai_alloc.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            winner_sum = torch.sum(ai_power_raw, dim=1, keepdim=True)\n",
    "            scale_factor = self.total_power / (winner_sum + 1e-12)\n",
    "            ai_power_final = ai_power_raw * scale_factor\n",
    "            \n",
    "        self.global_results[\"AI Model (Joint)\"].append(\n",
    "            self._calculate_high_fidelity_rates(channels, ai_alloc, ai_power_final))\n",
    "\n",
    "    def get_summary(self):\n",
    "        summary_data = []\n",
    "        for alg, rates_list in self.global_results.items():\n",
    "            if not rates_list: continue\n",
    "            all_rates = torch.cat(rates_list, dim=0) \n",
    "            \n",
    "            # 1. Sum Rate\n",
    "            avg_sum_rate = torch.mean(torch.sum(all_rates, dim=1)).item()\n",
    "            \n",
    "            # 2. Fairness (Global over K users)\n",
    "            K = all_rates.shape[1] \n",
    "            sum_r = torch.sum(all_rates, dim=1)\n",
    "            sum_r_sq = torch.sum(all_rates**2, dim=1)\n",
    "            \n",
    "            jain_samples = (sum_r**2) / (K * sum_r_sq + 1e-12)\n",
    "            avg_fairness = torch.mean(jain_samples).item()\n",
    "            \n",
    "            # 3. Edge Rate (5th Percentile of ALL users)\n",
    "            edge_rate = torch.quantile(all_rates.float(), 0.05).item()\n",
    "                \n",
    "            summary_data.append({\n",
    "                \"Algorithm\": alg,\n",
    "                \"Sum Rate\": avg_sum_rate,\n",
    "                \"Fairness\": avg_fairness,\n",
    "                \"Edge Rate\": edge_rate\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(summary_data).sort_values(\"Sum Rate\", ascending=False)\n",
    "\n",
    "def run_benchmark(model, test_loader, patch_cols, noise_var, device):\n",
    "    print(f\"Running Benchmark (Noise={noise_var:.1e})...\")\n",
    "    benchmarker = WirelessBenchmark(model, patch_cols, noise_variance=noise_var, device=device)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        channels = batch[0].to(device)\n",
    "        benchmarker.run_baselines(channels)\n",
    "        benchmarker.run_ai(channels)\n",
    "        \n",
    "    summary = benchmarker.get_summary()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL BENCHMARK RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "run_benchmark(full_model, test_loader, 4, noise_variance, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7abd0a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Joint Training...\n",
      "Epoch 1/50 | Temp: 5.00 | Train Loss: -2.0051\n",
      "Epoch 2/50 | Temp: 4.75 | Train Loss: -2.0814\n",
      "Epoch 3/50 | Temp: 4.51 | Train Loss: -2.2339\n",
      "Epoch 4/50 | Temp: 4.26 | Train Loss: -2.5164\n",
      "Epoch 5/50 | Temp: 4.02 | Train Loss: -3.0656\n",
      "   >> Validation Loss: -4.0803\n",
      "Epoch 6/50 | Temp: 3.77 | Train Loss: -4.1231\n",
      "Epoch 7/50 | Temp: 3.53 | Train Loss: -5.5099\n",
      "Epoch 8/50 | Temp: 3.29 | Train Loss: -7.1368\n",
      "Epoch 9/50 | Temp: 3.04 | Train Loss: -9.1425\n",
      "Epoch 10/50 | Temp: 2.79 | Train Loss: -11.4419\n",
      "   >> Validation Loss: -7.9193\n",
      "Epoch 11/50 | Temp: 2.55 | Train Loss: -13.7310\n",
      "Epoch 12/50 | Temp: 2.30 | Train Loss: -16.4275\n",
      "Epoch 13/50 | Temp: 2.06 | Train Loss: -19.2832\n",
      "Epoch 14/50 | Temp: 1.81 | Train Loss: -22.7844\n",
      "Epoch 15/50 | Temp: 1.57 | Train Loss: -26.8041\n",
      "   >> Validation Loss: -0.1292\n",
      "Epoch 16/50 | Temp: 1.32 | Train Loss: -31.2163\n",
      "Epoch 17/50 | Temp: 1.08 | Train Loss: -34.5708\n",
      "Epoch 18/50 | Temp: 0.83 | Train Loss: -37.4593\n",
      "Epoch 19/50 | Temp: 0.59 | Train Loss: -40.0084\n",
      "Epoch 20/50 | Temp: 0.34 | Train Loss: -41.0965\n",
      "   >> Validation Loss: -10.0485\n",
      "Epoch 21/50 | Temp: 0.10 | Train Loss: -42.2102\n",
      "Epoch 22/50 | Temp: 0.10 | Train Loss: -42.1217\n",
      "Epoch 23/50 | Temp: 0.10 | Train Loss: -42.2982\n",
      "Epoch 24/50 | Temp: 0.10 | Train Loss: -42.7850\n",
      "Epoch 25/50 | Temp: 0.10 | Train Loss: -43.0057\n",
      "   >> Validation Loss: -8.9703\n",
      "Epoch 26/50 | Temp: 0.10 | Train Loss: -42.9499\n",
      "Epoch 27/50 | Temp: 0.10 | Train Loss: -43.0518\n",
      "Epoch 28/50 | Temp: 0.10 | Train Loss: -43.1771\n",
      "Epoch 29/50 | Temp: 0.10 | Train Loss: -43.3266\n",
      "Epoch 30/50 | Temp: 0.10 | Train Loss: -43.6798\n",
      "   >> Validation Loss: -10.3079\n",
      "Epoch 31/50 | Temp: 0.10 | Train Loss: -44.0419\n",
      "Epoch 32/50 | Temp: 0.10 | Train Loss: -44.2491\n",
      "Epoch 33/50 | Temp: 0.10 | Train Loss: -44.2202\n",
      "Epoch 34/50 | Temp: 0.10 | Train Loss: -44.6520\n",
      "Epoch 35/50 | Temp: 0.10 | Train Loss: -44.5229\n",
      "   >> Validation Loss: -9.5067\n",
      "Epoch 36/50 | Temp: 0.10 | Train Loss: -44.7378\n",
      "Epoch 37/50 | Temp: 0.10 | Train Loss: -45.0115\n",
      "Epoch 38/50 | Temp: 0.10 | Train Loss: -45.2156\n",
      "Epoch 39/50 | Temp: 0.10 | Train Loss: -45.4480\n",
      "Epoch 40/50 | Temp: 0.10 | Train Loss: -45.8776\n",
      "   >> Validation Loss: -9.1132\n",
      "Epoch 41/50 | Temp: 0.10 | Train Loss: -45.6665\n",
      "Epoch 42/50 | Temp: 0.10 | Train Loss: -45.9323\n",
      "Epoch 43/50 | Temp: 0.10 | Train Loss: -46.3000\n",
      "Epoch 44/50 | Temp: 0.10 | Train Loss: -46.2897\n",
      "Epoch 45/50 | Temp: 0.10 | Train Loss: -46.6447\n",
      "   >> Validation Loss: -0.6376\n",
      "Epoch 46/50 | Temp: 0.10 | Train Loss: -46.8437\n",
      "Epoch 47/50 | Temp: 0.10 | Train Loss: -47.4170\n",
      "Epoch 48/50 | Temp: 0.10 | Train Loss: -47.6753\n",
      "Epoch 49/50 | Temp: 0.10 | Train Loss: -47.9163\n",
      "Epoch 50/50 | Temp: 0.10 | Train Loss: -48.1739\n",
      "   >> Validation Loss: -5.9489\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNNEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Deep Learning baseline: \n",
    "    Learns features directly from raw complex channels (Real/Imag).\n",
    "    \"\"\"\n",
    "    def __init__(self, num_antennas=1, num_subcarriers=64, embed_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input: (Batch, 2, Antennas, Subcarriers) - 2 for Real/Imag\n",
    "        self.conv_net = nn.Sequential(\n",
    "            # Layer 1\n",
    "            nn.Conv2d(2, 16, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(16, 32, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(32, 64, kernel_size=(1, 3), padding=(0, 1)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Global Average Pooling over Frequency/Antennas\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        \n",
    "        # Project to same embedding dimension as LWM (128)\n",
    "        self.projection = nn.Linear(64, embed_dim)\n",
    "\n",
    "    def forward(self, channels):\n",
    "        # channels shape: (B, 1, M, SC) complex\n",
    "        # Convert to (B, 2, M, SC) real\n",
    "        x = torch.cat([channels.real, channels.imag], dim=1)\n",
    "        \n",
    "        # Feature Extraction\n",
    "        features = self.conv_net(x) # (B, 64, 1, 1)\n",
    "        features = features.view(features.size(0), -1) # Flatten -> (B, 64)\n",
    "        \n",
    "        # Project\n",
    "        embeddings = self.projection(features) # (B, 128)\n",
    "        return embeddings\n",
    "\n",
    "class RawChannelModel(nn.Module):\n",
    "    def __init__(self, num_users, num_blocks, embed_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. The Baseline Encoder (Trainable)\n",
    "        self.encoder = SimpleCNNEncoder(embed_dim=embed_dim)\n",
    "        \n",
    "        # 2. The SAME Heads as the LWM Model\n",
    "        # (We assume CarrierAllocation class handles the heads internally)\n",
    "        # Note: We need to replicate the 'CarrierAllocation' logic here manually\n",
    "        # since CarrierAllocation expects an LWM encoder.\n",
    "        \n",
    "        self.patch_cols = 4 # Hardcoded for match\n",
    "        self.allocation_head = PowerAllocationHead(embed_dim=embed_dim, total_power=1.0)\n",
    "        \n",
    "        # Simple Assignment Head (Dot product attention style)\n",
    "        self.assignment_net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_blocks)\n",
    "        )\n",
    "\n",
    "    def forward(self, channels, temperature=1.0):\n",
    "        # channels: (B, K, M, SC)\n",
    "        B, K, M, SC = channels.shape\n",
    "        \n",
    "        # 1. Encode Each User Independently\n",
    "        # Flatten batch and users: (B*K, 1, M, SC)\n",
    "        channels_flat = channels.view(-1, 1, M, SC)\n",
    "        embeddings_flat = self.encoder(channels_flat)\n",
    "        \n",
    "        # Reshape back: (B, K, Embed_Dim)\n",
    "        user_embeddings = embeddings_flat.view(B, K, -1)\n",
    "        \n",
    "        # --- HEADS (Replicating LWM Logic) ---\n",
    "        \n",
    "        # 2. Assignment Head\n",
    "        # (B, K, Embed) -> (B, K, Blocks)\n",
    "        logits = self.assignment_net(user_embeddings) \n",
    "        \n",
    "        # Softmax over Users (dim 1) for each block\n",
    "        # \"Which user gets block b?\"\n",
    "        probs = F.softmax(logits / temperature, dim=1)\n",
    "        \n",
    "        # 3. Power Head (Using the specific class we wrote)\n",
    "        # Expand embeddings to blocks: (B, K, Blocks, Embed)\n",
    "        emb_expanded = user_embeddings.unsqueeze(2).expand(-1, -1, logits.shape[2], -1)\n",
    "        \n",
    "        # Predict Power\n",
    "        powers = self.allocation_head(emb_expanded, probs)\n",
    "        \n",
    "        return probs, powers\n",
    "    \n",
    "raw_model = RawChannelModel(8, 8)\n",
    "\n",
    "train_joint_scheduler(raw_model, train_loader, val_loader, noise_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d1d4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Benchmark (Noise=6.1e-10)...\n",
      "\n",
      "================================================================================\n",
      "FINAL BENCHMARK RESULTS\n",
      "================================================================================\n",
      "              Algorithm  Sum Rate (bps/Hz)  Fairness (Jain's)  Edge User Rate\n",
      "           Greedy (EPA)          63.265110           0.973548       12.656716\n",
      "       AI Model (Joint)          39.050255           0.968829        5.474275\n",
      "      Round Robin (EPA)          26.218662           0.538065        0.000505\n",
      "Proportional Fair (EPA)          25.770891           0.579959        0.000607\n"
     ]
    }
   ],
   "source": [
    "class WirelessBenchmark:\n",
    "    def __init__(self, model, noise_variance=1e-11, total_power=1.0, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.patch_cols = 4\n",
    "        self.noise = noise_variance\n",
    "        self.total_power = total_power\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "\n",
    "        self.global_results = {\n",
    "            \"Greedy (EPA)\": [], \"Round Robin (EPA)\": [], \n",
    "            \"Proportional Fair (EPA)\": [], \"AI Model (Joint)\": []\n",
    "        }\n",
    "\n",
    "    def _calculate_high_fidelity_rates(self, channels, allocation_map, power_map_per_block):\n",
    "        B, K, M, SC = channels.shape\n",
    "        Num_Blocks = allocation_map.shape[1]\n",
    "        SC_per_block = SC // Num_Blocks\n",
    "\n",
    "        # 1. Expand Block Decisions to Subcarriers\n",
    "        # (Batch, Num_Blocks) -> (Batch, SC)\n",
    "        alloc_sc = allocation_map.repeat_interleave(SC_per_block, dim=1)\n",
    "        \n",
    "        power_per_sc_val = power_map_per_block / SC_per_block\n",
    "        power_sc = power_per_sc_val.repeat_interleave(SC_per_block, dim=1)\n",
    "        \n",
    "        # 2. Physics: Calculate SNR per Subcarrier (MRC)\n",
    "        # (Batch, Users, SC)\n",
    "        channel_gains_sc = torch.sum(torch.abs(channels)**2, dim=2)\n",
    "        \n",
    "        # Gather winner gains\n",
    "        winner_gains_sc = torch.gather(channel_gains_sc, 1, alloc_sc.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # 3. Calculate Rate\n",
    "        snr_sc = (power_sc * winner_gains_sc) / self.noise\n",
    "        rate_sc = torch.log2(1 + snr_sc)\n",
    "        \n",
    "        # 4. Sum Rates per User\n",
    "        user_indices = torch.arange(K, device=self.device).view(1, K, 1)\n",
    "        mask = (alloc_sc.unsqueeze(1) == user_indices).float()\n",
    "        \n",
    "        # Result: (Batch, K) - Total Rate for each user in the batch\n",
    "        user_rates = torch.sum(rate_sc.unsqueeze(1) * mask, dim=2)\n",
    "        \n",
    "        return user_rates\n",
    "\n",
    "    def run_baselines(self, channels):\n",
    "        B, K, M, SC = channels.shape\n",
    "        num_blocks = SC // self.patch_cols\n",
    "        \n",
    "        # --- Pre-calculate Block Metrics for Decisions ---\n",
    "        # 1. Block Gains\n",
    "        mag_sq = torch.abs(channels)**2\n",
    "        gain_freq = torch.mean(mag_sq, dim=2)\n",
    "        gain_reshaped = gain_freq.view(B, K, num_blocks, self.patch_cols)\n",
    "        block_gains = torch.mean(gain_reshaped, dim=3)\n",
    "        \n",
    "        # 2. EPA Power\n",
    "        power_per_block = self.total_power / num_blocks\n",
    "        epa_map = torch.full((B, num_blocks), power_per_block, device=self.device)\n",
    "\n",
    "        # 3. Decision SNR (Linear)\n",
    "        block_snrs_est = (block_gains * power_per_block) / self.noise\n",
    "        \n",
    "        # 4. Decision Rates (Logarithmic)\n",
    "        block_rates_est = torch.log2(1 + block_snrs_est)\n",
    "\n",
    "        # --- Algorithm 1: Greedy ---\n",
    "        # Maximize Rate on each block\n",
    "        greedy_alloc = torch.argmax(block_rates_est, dim=1)\n",
    "        rates_greedy = self._calculate_high_fidelity_rates(channels, greedy_alloc, epa_map)\n",
    "        self.global_results[\"Greedy (EPA)\"].append(rates_greedy)\n",
    "\n",
    "        # --- Algorithm 2: Round Robin (Randomized Start) ---\n",
    "        # Fix: Randomize start index to prevent starvation of users K > N\n",
    "        start_offsets = torch.randint(0, K, (B, 1), device=self.device)\n",
    "        block_indices = torch.arange(num_blocks, device=self.device).unsqueeze(0) # (1, Blocks)\n",
    "        rr_alloc = (block_indices + start_offsets) % K\n",
    "        \n",
    "        rates_rr = self._calculate_high_fidelity_rates(channels, rr_alloc, epa_map)\n",
    "        self.global_results[\"Round Robin (EPA)\"].append(rates_rr)\n",
    "\n",
    "        # --- Algorithm 3: Proportional Fair (Corrected) ---\n",
    "        # Metric: Rate_est / Avg_Rate_est\n",
    "        # We approximate Avg_Rate_est as the mean rate available to the user across all blocks in this snapshot\n",
    "        user_avg_rate = torch.mean(block_rates_est, dim=2, keepdim=True)\n",
    "        pf_metric = block_rates_est / (user_avg_rate + 1e-8)\n",
    "        \n",
    "        pf_alloc = torch.argmax(pf_metric, dim=1)\n",
    "        rates_pf = self._calculate_high_fidelity_rates(channels, pf_alloc, epa_map)\n",
    "        self.global_results[\"Proportional Fair (EPA)\"].append(rates_pf)\n",
    "\n",
    "    def run_ai(self, channels):\n",
    "        # Normalize\n",
    "        user_pwr = torch.mean(torch.abs(channels)**2, dim=(2,3), keepdim=True)\n",
    "        user_scale = torch.sqrt(user_pwr + 1e-12)\n",
    "        channels_norm = channels / user_scale\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, powers = self.model(channels_norm, temperature=0.01)\n",
    "            \n",
    "            # Assignment\n",
    "            ai_alloc = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            # Power Extraction & Re-normalization\n",
    "            ai_power_raw = torch.gather(powers, 1, ai_alloc.unsqueeze(1)).squeeze(1)\n",
    "            winner_sum = torch.sum(ai_power_raw, dim=1, keepdim=True)\n",
    "            scale = self.total_power / (winner_sum + 1e-12)\n",
    "            ai_power_map = ai_power_raw * scale\n",
    "            \n",
    "        rates_ai = self._calculate_high_fidelity_rates(channels, ai_alloc, ai_power_map)\n",
    "        self.global_results[\"AI Model (Joint)\"].append(rates_ai)\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Computes global statistics across the entire dataset.\"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for alg_name, rate_list in self.global_results.items():\n",
    "            if not rate_list: continue\n",
    "            \n",
    "            # Concatenate all batches -> (Total_Samples, K)\n",
    "            all_rates = torch.cat(rate_list, dim=0)\n",
    "            \n",
    "            # Mask Ghosts globally\n",
    "            # We assume a user is \"active\" if they have non-zero rate potential in general\n",
    "            # For simplicity, we filter 0.0 rates if they are strictly 0 due to channel \n",
    "            # But safer: Filter based on rate > 1e-6 (Effective Zero)\n",
    "            active_rates = all_rates[all_rates > 1e-6] \n",
    "            \n",
    "            # 1. Sum Rate (Avg per cell)\n",
    "            # Sum over users, then Mean over samples\n",
    "            avg_sum_rate = torch.mean(torch.sum(all_rates, dim=1)).item()\n",
    "            \n",
    "            # 2. Fairness (Global Jain's)\n",
    "            # Calculated per sample, then averaged\n",
    "            sum_r = torch.sum(all_rates, dim=1)\n",
    "            sum_r_sq = torch.sum(all_rates**2, dim=1)\n",
    "            # Count active users per sample (users with Rate > 0)\n",
    "            n_active = torch.sum(all_rates > 1e-6, dim=1)\n",
    "            \n",
    "            # Avoid div/0 for empty samples\n",
    "            valid_mask = n_active > 0\n",
    "            jain_samples = (sum_r[valid_mask]**2) / (n_active[valid_mask] * sum_r_sq[valid_mask] + 1e-12)\n",
    "            avg_fairness = torch.mean(jain_samples).item()\n",
    "            \n",
    "            # 3. Edge Rate (True Global 5th Percentile)\n",
    "            edge_rate = torch.quantile(active_rates, 0.05).item()\n",
    "            \n",
    "            summary_data.append({\n",
    "                \"Algorithm\": alg_name,\n",
    "                \"Sum Rate (bps/Hz)\": avg_sum_rate,\n",
    "                \"Fairness (Jain's)\": avg_fairness,\n",
    "                \"Edge User Rate\": edge_rate\n",
    "            })\n",
    "            \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        return df.sort_values(by=\"Sum Rate (bps/Hz)\", ascending=False)\n",
    "\n",
    "def run_benchmark(model, test_loader, patch_cols, noise_var, device):\n",
    "    print(f\"Running Benchmark (Noise={noise_var:.1e})...\")\n",
    "    benchmarker = WirelessBenchmark(model, noise_variance=noise_var, device=device)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        channels = batch[0].to(device)\n",
    "        benchmarker.run_baselines(channels)\n",
    "        benchmarker.run_ai(channels)\n",
    "        \n",
    "    summary = benchmarker.get_summary()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL BENCHMARK RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "run_benchmark(raw_model, test_loader, 4, noise_variance, DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38458cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
