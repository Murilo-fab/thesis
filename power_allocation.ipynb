{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e54dcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.dataset import PowerAllocationDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54919f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 133979.64it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:03<00:00, 12517.61it/s]\n",
      "Generating Scenarios: 100%|██████████| 5000/5000 [00:05<00:00, 936.18it/s] \n"
     ]
    }
   ],
   "source": [
    "# General parameters\n",
    "SEED = 42\n",
    "INFERENCE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 5000\n",
    "NUM_USERS = [4][0]\n",
    "SCENARIO_NAME = \"city_6_miami\"\n",
    "BS_IDX = 1\n",
    "\n",
    "# Training Parameters\n",
    "EPOCHS = 15\n",
    "WARMUP_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3\n",
    "FINETUNE_LEARNING_RATE = 1e-5\n",
    "TRAINING_RATIOS = [1.0][0]\n",
    "\n",
    "D_MODEL = 128\n",
    "\n",
    "optimizer_config = {\"task_head_lr\": LEARNING_RATE,\n",
    "                    \"encoder_lr\": FINETUNE_LEARNING_RATE}\n",
    "\n",
    "dataset = PowerAllocationDataset(num_samples=NUM_SAMPLES,\n",
    "                                 num_users=NUM_USERS,\n",
    "                                 scenario_name=SCENARIO_NAME,\n",
    "                                 bs_idx=BS_IDX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f27646a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10441, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "877b96cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RefineBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1\n",
    "        self.bn1 = nn.BatchNorm2d(filters)\n",
    "        self.conv1 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "\n",
    "        # Layer 2\n",
    "        self.bn2 = nn.BatchNorm2d(filters)\n",
    "        self.conv2 = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "\n",
    "        # Path\n",
    "        out = self.bn1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        \n",
    "        out = self.bn2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        \n",
    "        # Residual Connection\n",
    "        out = out + shortcut\n",
    "        return out\n",
    "\n",
    "class BeamNetDecoder(nn.Module):\n",
    "    def __init__(self, num_users, num_carriers, num_antennas, embedding_dim, filters=64, p_max=1.0):\n",
    "        super(BeamNetDecoder, self).__init__()\n",
    "        \n",
    "        self.K = num_users\n",
    "        self.Nc = num_carriers\n",
    "        self.Nt = num_antennas\n",
    "        self.filters = filters\n",
    "        self.p_max = p_max\n",
    "        \n",
    "        # 1. Projection\n",
    "        # Input: (Batch, K, Emb) -> (Batch, K, Filters * Nc)\n",
    "        self.user_projection = nn.Linear(embedding_dim, filters * num_carriers)\n",
    "        \n",
    "        # 2. RefineNet Core (Same as before)\n",
    "        self.conv_input = nn.Conv2d(filters, filters, kernel_size=3, padding=1)\n",
    "        self.refine1 = RefineBlock(filters) # (defined in previous turn)\n",
    "        self.refine2 = RefineBlock(filters)\n",
    "        \n",
    "        # 3. Beam Head (Expansion)\n",
    "        # We need to output Real and Imag parts for every Antenna\n",
    "        # Output Channels = 2 * Nt\n",
    "        self.beam_head = nn.Conv2d(filters, 2 * num_antennas, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        B = z.size(0)\n",
    "        \n",
    "        # --- A. Expand & Reshape ---\n",
    "        x = self.user_projection(z)\n",
    "        x = x.view(B, self.K, self.filters, self.Nc)\n",
    "        x = x.permute(0, 2, 1, 3) # (Batch, Filters, Users, Carriers)\n",
    "        \n",
    "        # --- B. RefineNet (Interference Management) ---\n",
    "        x = self.conv_input(x)\n",
    "        x = self.refine1(x)\n",
    "        x = self.refine2(x)\n",
    "        \n",
    "        # --- C. Generate Raw Complex Vectors ---\n",
    "        # Shape: (Batch, 2*Nt, Users, Carriers)\n",
    "        raw_beams = self.beam_head(x)\n",
    "        \n",
    "        # Separate Real and Imaginary parts\n",
    "        # Reshape to: (Batch, Users, Carriers, Antennas, 2)\n",
    "        # 1. Permute to put channels last: (Batch, Users, Carriers, 2*Nt)\n",
    "        raw_beams = raw_beams.permute(0, 2, 3, 1)\n",
    "        \n",
    "        # 2. View to separate Real/Imag: (Batch, Users, Carriers, Nt, 2)\n",
    "        w = raw_beams.view(B, self.K, self.Nc, self.Nt, 2)\n",
    "        \n",
    "        # --- D. Power Constraint (L2 Normalization) ---\n",
    "        # Calculate Power per beam: |w|^2 = Re^2 + Im^2\n",
    "        # Sum over Antennas (dim 3) and Re/Im (dim 4)\n",
    "        beam_power = torch.sum(w ** 2, dim=(3, 4), keepdim=True) # (B, K, Nc, 1, 1)\n",
    "        \n",
    "        # Total Power used in the system (sum over Users and Carriers)\n",
    "        total_system_power = torch.sum(beam_power, dim=(1, 2), keepdim=True)\n",
    "        \n",
    "        # Scale factor to satisfy P_max\n",
    "        # We clamp the divisor to avoid exploding gradients if power is near 0\n",
    "        scale = torch.sqrt(self.p_max / (total_system_power + 1e-8))\n",
    "        \n",
    "        # Apply scaling\n",
    "        w_final = w * scale\n",
    "        \n",
    "        w_final = w_final.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        \n",
    "        return w_final\n",
    "    \n",
    "class Wrapper(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 task_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = model\n",
    "        self.task_head = task_head\n",
    "\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def fine_tune(self, fine_tune_layers=\"full\"):\n",
    "        if fine_tune_layers == \"full\":\n",
    "             for param in self.encoder.parameters():\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            for name, param in self.encoder.named_parameters():\n",
    "                if any(layer in name for layer in fine_tune_layers):\n",
    "                    param.requires_grad = True\n",
    "         \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"\n",
    "        Forward function for Wrapper\n",
    "        \n",
    "        Inputs:\n",
    "        channels (torch.tensor): Channel matrix [B, S, K, N]\n",
    "\n",
    "        Outputs:\n",
    "        power_weights (torch.tensor): Normalized weights [B, S, K]\n",
    "        \"\"\"\n",
    "        # 1. Extract channel shape\n",
    "        B, K, S, F = tokens.shape\n",
    "\n",
    "        # 2. Flatten for Encoder\n",
    "        # Shape: [B, K, S, F] : [B*K, S, F]\n",
    "        x = tokens.view(B*K, S, F)\n",
    "\n",
    "        # 3. Encoder\n",
    "        # Embeddings shape: [B*K, S, d_model]\n",
    "        embeddings, _ = self.encoder(x)\n",
    "\n",
    "        # 4. Extract CLS Token\n",
    "        # Shape: [B*K, d_model]\n",
    "        cls_embedding = embeddings[:, 0, :]\n",
    "\n",
    "        # 5. Reshape to [Batch, Users, D] BEFORE the head\n",
    "        # This is the key change. We reconstruct the user dimension here.\n",
    "        cls_structured = cls_embedding.view(B, K, -1)\n",
    "\n",
    "        # 6. Head Pass\n",
    "        # The head now takes the structured data and returns normalized power\n",
    "        W = self.task_head(cls_structured)\n",
    "\n",
    "        return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a8bcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BeamformingSumRateLoss(nn.Module):\n",
    "    def __init__(self, P_total=1.0, noise_variance=1.0):\n",
    "        super().__init__()\n",
    "        self.P_total = P_total\n",
    "        self.noise_variance = noise_variance\n",
    "\n",
    "    def forward(self, W_pred, H):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            W_pred: Predicted Beamforming Vectors [Batch, S, K, N] (Complex)\n",
    "                    or [Batch, S, K, N, 2] (Real, Imag)\n",
    "            H:      Channel Matrix [Batch, S, K, N] (Complex)\n",
    "                    or [Batch, S, K, N, 2] (Real, Imag)\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. Prepare Complex Tensors ---\n",
    "        if W_pred.shape[-1] == 2:\n",
    "            W_complex = torch.view_as_complex(W_pred)\n",
    "        else:\n",
    "            W_complex = W_pred\n",
    "            \n",
    "        if H.shape[-1] == 2:\n",
    "            H_complex = torch.view_as_complex(H)\n",
    "        else:\n",
    "            H_complex = H\n",
    "\n",
    "        # --- 2. Enforce System-Wide Power Constraint ---\n",
    "        # Calculate total power used in the current prediction\n",
    "        # Sum over: Subcarriers (1), Users (2), Antennas (3)\n",
    "        # Result shape: [Batch, 1, 1, 1] for broadcasting\n",
    "        current_total_power = torch.sum(W_complex.abs() ** 2, dim=(1, 2, 3), keepdim=True)\n",
    "        \n",
    "        # Scale W to match P_total exactly\n",
    "        # If the model output is already normalized, this scaling factor will be 1.0 (no change).\n",
    "        # We add epsilon to avoid division by zero.\n",
    "        scaling_factor = torch.sqrt(self.P_total / (current_total_power + 1e-8))\n",
    "        \n",
    "        W_normalized = W_complex * scaling_factor\n",
    "\n",
    "        # --- 3. Compute Interaction Matrix (Signal & Interference) ---\n",
    "        # We want h_k^H * w_j for all k, j.\n",
    "        # H_complex: [B, S, K, N]\n",
    "        # W_normalized: [B, S, K, N] -> Transpose to [B, S, N, K]\n",
    "        \n",
    "        # Result: [B, S, K, K]\n",
    "        # Element [b, s, i, j] = Signal at User i from Beam j\n",
    "        interaction_matrix = torch.matmul(H_complex.conj(), W_normalized.transpose(-2, -1))\n",
    "        \n",
    "        # Power = Magnitude Squared\n",
    "        power_matrix = interaction_matrix.abs() ** 2\n",
    "\n",
    "        # --- 4. Extract Signal vs Interference ---\n",
    "        # Diagonal elements (i=j) are the desired signals\n",
    "        signal_power = torch.diagonal(power_matrix, dim1=-2, dim2=-1) # [B, S, K]\n",
    "        \n",
    "        # Sum of rows is Total Received Power at User i\n",
    "        total_received_power = torch.sum(power_matrix, dim=-1) # [B, S, K]\n",
    "        \n",
    "        # Interference = Total - Signal\n",
    "        interference_power = total_received_power - signal_power\n",
    "        \n",
    "        # --- 5. Calculate SINR & Rate ---\n",
    "        # SINR = Signal / (Interference + Noise)\n",
    "        sinr = signal_power / (interference_power + self.noise_variance + 1e-10)\n",
    "        \n",
    "        # Rate = log2(1 + SINR)\n",
    "        rate = torch.log2(1 + sinr)\n",
    "\n",
    "        # --- 6. Sum Rate ---\n",
    "        # Sum over Users and Subcarriers\n",
    "        sum_rate_per_sample = torch.sum(rate, dim=(1, 2))\n",
    "        \n",
    "        # Average over the batch (Minimize negative rate)\n",
    "        loss = -torch.mean(sum_rate_per_sample)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LWM model...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m model = Wrapper(lwm_model, task_head).to(INFERENCE_DEVICE)\n\u001b[32m     21\u001b[39m criterion = SumRateLoss(P_TOTAL, NOISE_VARIANCE)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model= \u001b[43mtrain_downstream_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfraction_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m                              \u001b[49m\u001b[43moptimizer_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINFERENCE_DEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m                              \u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m#print(benchmark(model, test_loader, P_TOTAL, NOISE_VARIANCE, INFERENCE_DEVICE))\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/src/train.py:152\u001b[39m, in \u001b[36mtrain_downstream_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer_config, criterion, epochs, device, results_folder)\u001b[39m\n\u001b[32m    149\u001b[39m optimizer.zero_grad()\n\u001b[32m    150\u001b[39m pred = model(batch_tokens)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m loss.backward()\n\u001b[32m    154\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/src/metrics.py:33\u001b[39m, in \u001b[36mSumRateLoss.forward\u001b[39m\u001b[34m(self, pred_power, H)\u001b[39m\n\u001b[32m     30\u001b[39m     V_mrt = get_mrt_directions(H)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# 3. Combine power and directions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m W_pred = \u001b[43mV_mrt\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpower\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 4. Calculate sum rate\u001b[39;00m\n\u001b[32m     36\u001b[39m sum_rate_batch = calculate_sum_rate(H, W_pred, \u001b[38;5;28mself\u001b[39m.noise_variance)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (32) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from src.utils import prepare_loaders, get_subset, load_lwm_model\n",
    "from src.lwm_model import lwm\n",
    "from src.downstream_models import RegressionHead, Wrapper\n",
    "from src.metrics import SumRateLoss, benchmark\n",
    "from src.train import train_downstream_model\n",
    "\n",
    "P_TOTAL = 1.0\n",
    "NOISE_VARIANCE = 1e-3\n",
    "\n",
    "channels = dataset.raw_channels.permute(0, 3, 1, 2) * 1e6\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_loaders(channels, dataset.data_tokens, seed=SEED)\n",
    "\n",
    "results_folder = f\"./results/new_test\"\n",
    "fraction_train_loader = get_subset(train_loader, TRAINING_RATIOS, seed=SEED)\n",
    "\n",
    "lwm_model = load_lwm_model(lwm(), \"./models/model.pth\", INFERENCE_DEVICE)\n",
    "task_head = RegressionHead(D_MODEL, num_subcarriers=32)\n",
    "\n",
    "model = Wrapper(lwm_model, task_head).to(INFERENCE_DEVICE)\n",
    "\n",
    "criterion = SumRateLoss(P_TOTAL, NOISE_VARIANCE)\n",
    "\n",
    "model= train_downstream_model(model=model,\n",
    "                              train_loader=fraction_train_loader,\n",
    "                              val_loader=val_loader,\n",
    "                              optimizer_config=optimizer_config,\n",
    "                              criterion=criterion,\n",
    "                              epochs=EPOCHS,\n",
    "                              device=INFERENCE_DEVICE,\n",
    "                              results_folder=results_folder)\n",
    "\n",
    "#print(benchmark(model, test_loader, P_TOTAL, NOISE_VARIANCE, INFERENCE_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d96621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LWM model...\n",
      "Model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [08:03<00:00, 32.25s/it, Train Loss=-390, Validation Loss=-390, LR=0.001]\n",
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     21\u001b[39m criterion = BeamformingSumRateLoss(NOISE_VARIANCE)\n\u001b[32m     23\u001b[39m model= train_downstream_model(model=model,\n\u001b[32m     24\u001b[39m                               train_loader=fraction_train_loader,\n\u001b[32m     25\u001b[39m                               val_loader=val_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m                               device=INFERENCE_DEVICE,\n\u001b[32m     30\u001b[39m                               results_folder=results_folder)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_TOTAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNOISE_VARIANCE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mINFERENCE_DEVICE\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/src/metrics.py:82\u001b[39m, in \u001b[36mbenchmark\u001b[39m\u001b[34m(model, dataloader, P_total, noise_variance, device)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# 3.2 Method 1: ML - MRT Directions + Model Power\u001b[39;00m\n\u001b[32m     79\u001b[39m \n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Construct precoder\u001b[39;00m\n\u001b[32m     81\u001b[39m V_mrt = get_mrt_directions(H)\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m W_mrt_ml = \u001b[43mV_mrt\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mml_power\u001b[49m\u001b[43m.\u001b[49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Calculate rate\u001b[39;00m\n\u001b[32m     85\u001b[39m rate_mrt_ml_batch = calculate_sum_rate(H, W_mrt_ml, noise_variance)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (2) at non-singleton dimension 5"
     ]
    }
   ],
   "source": [
    "from src.utils import prepare_loaders, get_subset, load_lwm_model\n",
    "from src.lwm_model import lwm\n",
    "from src.metrics import SumRateLoss, benchmark\n",
    "from src.train import train_downstream_model\n",
    "\n",
    "P_TOTAL = 1.0\n",
    "NOISE_VARIANCE = 1e-3\n",
    "\n",
    "channels = dataset.raw_channels.permute(0, 3, 1, 2) * 1e6\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_loaders(channels, dataset.data_tokens, seed=SEED)\n",
    "\n",
    "results_folder = f\"./results/new_test_csi_net\"\n",
    "fraction_train_loader = get_subset(train_loader, TRAINING_RATIOS, seed=SEED)\n",
    "\n",
    "lwm_model = load_lwm_model(lwm(), \"./models/model.pth\", INFERENCE_DEVICE)\n",
    "task_head = BeamNetDecoder(NUM_USERS, 32, 16, D_MODEL).to(INFERENCE_DEVICE)\n",
    "\n",
    "model = Wrapper(lwm_model, task_head).to(INFERENCE_DEVICE)\n",
    "\n",
    "criterion = BeamformingSumRateLoss(NOISE_VARIANCE)\n",
    "\n",
    "model= train_downstream_model(model=model,\n",
    "                              train_loader=fraction_train_loader,\n",
    "                              val_loader=val_loader,\n",
    "                              optimizer_config=optimizer_config,\n",
    "                              criterion=criterion,\n",
    "                              epochs=EPOCHS,\n",
    "                              device=INFERENCE_DEVICE,\n",
    "                              results_folder=results_folder)\n",
    "\n",
    "# print(benchmark(model, test_loader, P_TOTAL, NOISE_VARIANCE, INFERENCE_DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9427332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_noise_power(bandwidth_ghz, noise_figure_db=9):\n",
    "    \"\"\"\n",
    "    Calculates noise variance (sigma^2) in linear scale (Watts).\n",
    "    \"\"\"\n",
    "    k_B = 1.380649e-23  # Boltzmann constant\n",
    "    T = 290             # Temperature (Kelvin)\n",
    "    BW_Hz = bandwidth_ghz * 1e9 # Convert GHz to Hz\n",
    "    \n",
    "    # Thermal Noise Density (N0)\n",
    "    noise_spectral_density = k_B * T \n",
    "    \n",
    "    # Noise Figure in Linear Scale\n",
    "    noise_figure_linear = 10 ** (noise_figure_db / 10)\n",
    "    \n",
    "    # Total Noise Power\n",
    "    noise_power_watts = noise_spectral_density * BW_Hz * noise_figure_linear\n",
    "    \n",
    "    return noise_power_watts\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
