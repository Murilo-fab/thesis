{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54919f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from faker import Faker\n",
    "from itertools import product\n",
    "\n",
    "# General parameters\n",
    "SEED = 42\n",
    "INFERENCE_DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset parameters\n",
    "NUM_SAMPLES = 10000\n",
    "SCENARIO_NAME = \"city_6_miami\"\n",
    "BS_IDX = 1\n",
    "\n",
    "# Simulation parameters\n",
    "P_TOTAL = 20.0\n",
    "NOISE_VARIANCE = 1e-12\n",
    "\n",
    "# LWM Model Parameters\n",
    "D_MODEL = 128\n",
    "MODEL_PATH = \"./models/model.pth\"\n",
    "\n",
    "# Training Parameters\n",
    "EPOCHS = 25\n",
    "WARMUP_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "FINETUNE_LEARNING_RATE = 1e-5\n",
    "\n",
    "optimizer_configs = {\"task_head_lr\": LEARNING_RATE,\n",
    "                    \"encoder_lr\": FINETUNE_LEARNING_RATE}\n",
    "\n",
    "NUM_USERS = [4, 8, 12, 16]\n",
    "PATCH_SIZE = [\n",
    "    {\"patch_cols\": 2, \"patch_rows\": 2},\n",
    "    {\"patch_cols\": 4, \"patch_rows\": 4},\n",
    "    {\"patch_cols\": 16, \"patch_rows\": 1}, # One Token per carrier\n",
    "    ]\n",
    "TRAINING_RATIOS = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "HIDDEN_DIMS = [64, 128, 256, 512, 1024]\n",
    "FINETUNE_STATUS = [None, [\"layers.8\", \"layers.9\", \"layers.10\", \"layers.11\"], \"full\"]\n",
    "INPUT_TYPES = [\"cls\", \"channel_emb\", \"raw\"]\n",
    "PRECODERS = [\"mrt\", \"zf\"]\n",
    "\n",
    "experiments = [\n",
    "    {\n",
    "        \"num_users\": num_users,\n",
    "        \"patch_rows\": patch[\"patch_rows\"],\n",
    "        \"patch_cols\": patch[\"patch_cols\"],\n",
    "        \"training_ratio\": train_ratio,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"finetune\": finetune,\n",
    "        \"input_type\": input_type,\n",
    "        \"precoder\": precoder\n",
    "    }\n",
    "    for num_users, patch, train_ratio, hidden_dim, finetune, input_type, precoder\n",
    "    in product(\n",
    "        NUM_USERS,\n",
    "        PATCH_SIZE,\n",
    "        TRAINING_RATIOS,\n",
    "        HIDDEN_DIMS,\n",
    "        FINETUNE_STATUS,\n",
    "        INPUT_TYPES,\n",
    "        PRECODERS\n",
    "    )\n",
    "]\n",
    "\n",
    "# Initialize with the German locale\n",
    "fake = Faker('de_DE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4174c4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Basestation 1\n",
      "\n",
      "UE-BS Channels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading ray-tracing: 100%|██████████| 42984/42984 [00:00<00:00, 149417.92it/s]\n",
      "Generating channels: 100%|██████████| 42984/42984 [00:04<00:00, 10479.41it/s]\n",
      "Generating Scenarios: 100%|██████████| 10000/10000 [00:11<00:00, 881.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./models/model.pth\n",
      "* Created new run: Run_2025-12-27_00-53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [00:17<00:04,  1.11it/s, Train Loss=-18.9, Validation Loss=-18.5, LR=0.001]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     44\u001b[39m trackio_params = {\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproject\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpower_allocation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgroup\u001b[39m\u001b[33m\"\u001b[39m: PRECODER,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     }\n\u001b[32m     53\u001b[39m }\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parameters[\u001b[33m\"\u001b[39m\u001b[33mfinetune\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m parameters[\u001b[33m\"\u001b[39m\u001b[33minput_type\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     model= \u001b[43mtrain_downstream_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfraction_train_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43moptimizer_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_configs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mINFERENCE_DEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresults_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m                                  \u001b[49m\u001b[43mtrackio_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrackio_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     65\u001b[39m     model = finetune(model=model,\n\u001b[32m     66\u001b[39m                      train_loader=fraction_train_loader,\n\u001b[32m     67\u001b[39m                      val_loader=val_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m                      save_dir=results_folder,\n\u001b[32m     75\u001b[39m                      trackio_params=trackio_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/src/train.py:272\u001b[39m, in \u001b[36mtrain_downstream_model\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer_configs, criterion, epochs, device, save_dir, trackio_params)\u001b[39m\n\u001b[32m    268\u001b[39m         batch_tokens = batch[\u001b[32m1\u001b[39m].to(device)\n\u001b[32m    270\u001b[39m         pred = model(batch_tokens)\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m         loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m         val_loss += loss.item()\n\u001b[32m    275\u001b[39m val_loss /= \u001b[38;5;28mlen\u001b[39m(val_loader)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/src/metrics.py:255\u001b[39m, in \u001b[36mSumRateLoss.forward\u001b[39m\u001b[34m(self, pred_power, H)\u001b[39m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    254\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.precoder == \u001b[33m\"\u001b[39m\u001b[33mzf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m         V = \u001b[43mget_zf_directions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.precoder == \u001b[33m\"\u001b[39m\u001b[33mmrt\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m         V = get_mrt_directions(H)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/thesis/src/power_allocation.py:194\u001b[39m, in \u001b[36mget_zf_directions\u001b[39m\u001b[34m(H)\u001b[39m\n\u001b[32m    192\u001b[39m gram = H @ H_hermitian\n\u001b[32m    193\u001b[39m eye = torch.eye(gram.shape[-\u001b[32m1\u001b[39m], device=H.device, dtype=H.dtype)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m gram_inv = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgram\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43meye\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;66;03m# 1.3 ZF Precoder\u001b[39;00m\n\u001b[32m    197\u001b[39m W_raw = H_hermitian @ gram_inv\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "from src.dataset import PowerAllocationDataset\n",
    "from src.utils import prepare_loaders, get_subset\n",
    "from src.lwm_model import lwm\n",
    "from src.downstream_models import PowerAllocator, Wrapper\n",
    "from src.metrics import SumRateLoss, Benchmark, EmbeddingAnalyzer\n",
    "from src.train import train_downstream_model, finetune\n",
    "\n",
    "for parameters in experiments:\n",
    "    USERS = parameters['num_users']\n",
    "    PATCH_ROWS = parameters[\"patch_rows\"]\n",
    "    PATCH_COLS = parameters[\"patch_cols\"]\n",
    "    TRAINING_RATIO = parameters[\"training_ratio\"]\n",
    "    HIDDEN_DIM = parameters[\"hidden_dim\"]\n",
    "    FINETUNE = parameters[\"finetune\"] if INPUT_TYPE != \"raw\" else None\n",
    "    INPUT_TYPE = parameters[\"input_type\"]\n",
    "    PRECODER = parameters[\"precoder\"]\n",
    "    \n",
    "    experiment_name = fake.city()\n",
    "    results_folder = f\"./results/{experiment_name}\"\n",
    "\n",
    "    dataset = PowerAllocationDataset(num_samples=NUM_SAMPLES,\n",
    "                                     num_users=USERS,\n",
    "                                     scenario_name=SCENARIO_NAME,\n",
    "                                     bs_idx=BS_IDX,\n",
    "                                     patch_cols=PATCH_COLS,\n",
    "                                     patch_rows=PATCH_ROWS)\n",
    "    \n",
    "    NUM_SUBCARRIERS = dataset.generator.n_subcarriers\n",
    "    \n",
    "    train_loader, val_loader, test_loader = prepare_loaders(dataset.raw_channels, dataset.data_tokens, seed=SEED)\n",
    "    fraction_train_loader = get_subset(train_loader, TRAINING_RATIO, seed=SEED)\n",
    "\n",
    "    lwm_model = lwm.from_pretrained(ckpt_name=MODEL_PATH, device=INFERENCE_DEVICE)\n",
    "    if INPUT_TYPE == \"raw\":\n",
    "        task_head = PowerAllocator(32, num_subcarriers=NUM_SUBCARRIERS, hidden_dim=HIDDEN_DIM)\n",
    "    else:\n",
    "        task_head = PowerAllocator(D_MODEL, num_subcarriers=NUM_SUBCARRIERS, hidden_dim=HIDDEN_DIM)\n",
    "\n",
    "    model = Wrapper(lwm_model, task_head, input_type=parameters[\"input_type\"]).to(INFERENCE_DEVICE)\n",
    "\n",
    "    criterion = SumRateLoss(P_TOTAL, NOISE_VARIANCE, precoder=PRECODER)\n",
    "\n",
    "    trackio_params = {\n",
    "        \"project\": \"power_allocation\",\n",
    "        \"group\": PRECODER,\n",
    "        \"config\": {\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"device\": INFERENCE_DEVICE,\n",
    "            \"p_total\": P_TOTAL,\n",
    "            \"noise_variance\": NOISE_VARIANCE\n",
    "        }\n",
    "    }\n",
    "    if parameters[\"finetune\"] == None or parameters[\"input_type\"] == \"raw\":\n",
    "        model= train_downstream_model(model=model,\n",
    "                                      train_loader=fraction_train_loader,\n",
    "                                      val_loader=val_loader,\n",
    "                                      optimizer_configs=optimizer_configs,\n",
    "                                      criterion=criterion,\n",
    "                                      epochs=EPOCHS,\n",
    "                                      device=INFERENCE_DEVICE,\n",
    "                                      save_dir=results_folder,\n",
    "                                      trackio_params=trackio_params)\n",
    "    else:\n",
    "        model = finetune(model=model,\n",
    "                         train_loader=fraction_train_loader,\n",
    "                         val_loader=val_loader,\n",
    "                         fine_tune_layers=FINETUNE,\n",
    "                         optimizer_configs=optimizer_configs,\n",
    "                         criterion=criterion,\n",
    "                         epochs=EPOCHS,\n",
    "                         warmup_epochs=WARMUP_EPOCHS,\n",
    "                         device=INFERENCE_DEVICE,\n",
    "                         save_dir=results_folder,\n",
    "                         trackio_params=trackio_params)\n",
    "        \n",
    "    with open(f\"{results_folder}/parameters.json\", \"w\") as f:\n",
    "        json.dump(parameters, f, indent=4)\n",
    "\n",
    "    benchmark = Benchmark(test_loader, P_TOTAL, NOISE_VARIANCE, precoder=PRECODER)\n",
    "    benchmark_results = benchmark.evaluate(model)\n",
    "    benchmark.plot_cdf(benchmark_results, results_folder)\n",
    "    if INPUT_TYPE != \"raw\":\n",
    "        analyzer = EmbeddingAnalyzer(model, INFERENCE_DEVICE)\n",
    "        analyzer.run(test_loader, results_dir=results_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8daab26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
