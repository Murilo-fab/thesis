{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e7e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from thesis.utils import prepare_loaders\n",
    "from thesis.dataset import DeepMIMOGenerator\n",
    "\n",
    "num_users = 8\n",
    "\n",
    "generator = DeepMIMOGenerator()\n",
    "total_samples = 5000\n",
    "distribution = [(int(round(0.1*total_samples)), 0, 0.7, 20),\n",
    "                (int(round(0.45*total_samples)), 0, 0.5, 10000),\n",
    "                (int(round(0.45*total_samples)), 0.8, 0.99, 10)]\n",
    "\n",
    "datasets = []\n",
    "\n",
    "for samples, min_corr, max_corr, max_gain in distribution:\n",
    "    dataset, _ = generator.generate_dataset(samples, num_users, min_corr, max_corr, max_gain)\n",
    "    datasets.append(dataset)\n",
    "\n",
    "final_dataset = np.concatenate(datasets, axis=0)\n",
    "dataset_tensor = torch.tensor(final_dataset)\n",
    "\n",
    "train_loader, val_loader, test_loader = prepare_loaders(dataset_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7f4c703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_noise_variance(channels, target_snr_db=5.0, mode='edge', percentile=0.05):\n",
    "    \"\"\"\n",
    "    Calculates the noise variance required to achieve a specific SNR target.\n",
    "    \n",
    "    Args:\n",
    "        channels: (Batch, Users, M, SC) Complex tensor\n",
    "        target_snr_db: Desired SNR in decibels (e.g., 5.0)\n",
    "        mode: 'mean' (targets average user) or 'edge' (targets weak users)\n",
    "        percentile: The cutoff for 'edge' mode (default 0.05 = 5th percentile)\n",
    "        \n",
    "    Returns:\n",
    "        noise_variance: float\n",
    "    \"\"\"\n",
    "    # 1. Calculate Signal Power per User\n",
    "    # We average over Antennas (dim 2) and Carriers (dim 3) to get one power value per user\n",
    "    # Shape: (Batch, Users)\n",
    "    user_powers = torch.mean(torch.abs(channels)**2, dim=(2, 3))\n",
    "    \n",
    "    # Flatten to see the global distribution of all users in the batch\n",
    "    all_powers = user_powers.flatten()\n",
    "    \n",
    "    # 2. Determine Reference Signal Power\n",
    "    if mode == 'mean':\n",
    "        # Calibrate noise so the AVERAGE user has the target SNR\n",
    "        # Result: Weak users might still be dead (< 0dB)\n",
    "        ref_power = torch.mean(all_powers).item()\n",
    "        \n",
    "    elif mode == 'edge':\n",
    "        # Calibrate noise so the WEAKEST users (5th percentile) have the target SNR\n",
    "        # Result: Weak users are alive, Strong users have very high SNR\n",
    "        ref_power = torch.quantile(all_powers, percentile).item()\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode: {mode}\")\n",
    "        \n",
    "    # 3. Calculate Required Noise\n",
    "    # SNR_linear = Signal_Power / Noise_Power\n",
    "    # Noise_Power = Signal_Power / SNR_linear\n",
    "    \n",
    "    target_snr_linear = 10 ** (target_snr_db / 10.0)\n",
    "    noise_variance = ref_power / target_snr_linear\n",
    "    \n",
    "    return noise_variance\n",
    "\n",
    "noise_variance = calculate_noise_variance(dataset_tensor, mode=\"mean\", target_snr_db=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "420bf490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import  torch.nn.functional as F\n",
    "\n",
    "class AssignmentHead(nn.Module):\n",
    "    def __init__(self, emb_dim=128, hidden_dim=64):\n",
    "        super().__init__()\n",
    "\n",
    "        # MLP Net\n",
    "        # Input: (Batch, Users, Num_Blocks, Emb_dim)\n",
    "        # Output: (Batch, User, Num_Blocks, 1)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, user_embeddings, temperature=1.0):\n",
    "        \n",
    "        scores = self.net(user_embeddings)\n",
    "        scores = scores.squeeze(-1)\n",
    "        scaled_scores = scores / temperature\n",
    "        assignment_probs = F.softmax(scaled_scores, dim=1)\n",
    "\n",
    "        return assignment_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b235470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerAllocationHead(nn.Module):\n",
    "    def __init__(self, embed_dim=128, hidden_dim=64, total_power=1.0):\n",
    "        super().__init__()\n",
    "        self.total_power = total_power\n",
    "        \n",
    "        # Increase input dimension to include assignment probability\n",
    "        input_dim = embed_dim + 1 \n",
    "        \n",
    "        self.power_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1), \n",
    "            nn.Sigmoid() # Raw \"scores\" in [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, embeddings, assignment_probs):\n",
    "        # 1. Prepare Input\n",
    "        probs_expanded = assignment_probs.unsqueeze(-1)\n",
    "        x = torch.cat([embeddings, probs_expanded], dim=-1)\n",
    "        \n",
    "        # 2. Predict Raw Scores (Importance of each block)\n",
    "        # Add epsilon to prevent all-zeros\n",
    "        raw_scores = self.power_net(x).squeeze(-1) + 1e-8\n",
    "        \n",
    "        # 3. FORCE FULL POWER USAGE (Softmax-style normalization)\n",
    "        # Sum of scores per sample\n",
    "        total_score = torch.sum(raw_scores, dim=(1, 2), keepdim=True)\n",
    "        \n",
    "        # Normalize: fractions always sum to 1.0\n",
    "        power_fractions = raw_scores / total_score\n",
    "        \n",
    "        # Scale to Budget\n",
    "        allocated_power = power_fractions * self.total_power\n",
    "        \n",
    "        return allocated_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26db29dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JointUtilityLoss(nn.Module):\n",
    "    def __init__(self, alpha_entropy=0.01, alpha_power=0.001, noise_var=1e-9):\n",
    "        super().__init__()\n",
    "        self.alpha_entropy = alpha_entropy\n",
    "        self.alpha_power = alpha_power\n",
    "        self.noise_var = noise_var\n",
    "\n",
    "    def forward(self, assignment_probs, power_values, channels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            assignment_probs: (Batch, Users, Blocks) - Model Decision\n",
    "            power_values:     (Batch, Users, Blocks) - Model Decision\n",
    "            channels:         (Batch, Users, Ant, SC) - REAL Physics\n",
    "        \"\"\"\n",
    "        B, K, M, SC = channels.shape\n",
    "        _, _, Blocks = assignment_probs.shape\n",
    "        \n",
    "        # 1. The Mapping Problem: Blocks -> Subcarriers\n",
    "        # We need to stretch the model's decision to match the physical subcarriers\n",
    "        subcarriers_per_block = SC // Blocks\n",
    "        \n",
    "        # Expand: (B, K, Blocks) -> (B, K, Blocks, SC_per_Block) -> (B, K, SC)\n",
    "        # This repeats the block decision for every subcarrier inside it\n",
    "        A_full = assignment_probs.repeat_interleave(subcarriers_per_block, dim=2)\n",
    "        P_full = power_values.repeat_interleave(subcarriers_per_block, dim=2)\n",
    "        \n",
    "        # 2. Physics: Handle Antennas (M)\n",
    "        # If we are doing Single-Stream Beamforming (MRC/MRT), \n",
    "        # the effective channel gain is the sum of magnitudes squared across antennas.\n",
    "        # This assumes optimal receiver combining.\n",
    "        # Shape: (B, K, SC)\n",
    "        channel_gains_full = torch.sum(torch.abs(channels)**2, dim=2)\n",
    "        \n",
    "        # 3. Calculate REAL SINR (Per Subcarrier)\n",
    "        # Signal = A_sc * P_sc * Gain_sc\n",
    "        signal_power = A_full * P_full * channel_gains_full\n",
    "        \n",
    "        # Interference (Per Subcarrier)\n",
    "        # Total power on this subcarrier from ALL users\n",
    "        total_power_on_sc = torch.sum(signal_power, dim=1, keepdim=True)\n",
    "        interference = total_power_on_sc - signal_power\n",
    "        \n",
    "        # SINR is now calculated using the EXACT gain of specific subcarriers\n",
    "        # (No averaging happened yet!)\n",
    "        sinr = signal_power / (interference + self.noise_var)\n",
    "        \n",
    "        # 4. Calculate Rate (Per Subcarrier)\n",
    "        # The log happens HERE, capturing the true capacity\n",
    "        # Rate = A * log(1 + SINR)\n",
    "        sc_rates = A_full * torch.log2(1 + sinr)\n",
    "        \n",
    "        # 5. Aggregate\n",
    "        # Now we sum up the rates of all subcarriers to get the user's total rate\n",
    "        user_total_rate = torch.sum(sc_rates, dim=2) \n",
    "        \n",
    "        # --- Fairness / Utility Calculation ---\n",
    "        # Proportional Fair: Mean( Log( User_Rate ) )\n",
    "        log_utility = torch.log(user_total_rate + 1e-8)\n",
    "        loss_utility = -torch.mean(torch.sum(log_utility, dim=1))\n",
    "        \n",
    "        # Regularization (calculated on the compact block outputs to save RAM)\n",
    "        entropy = -torch.sum(assignment_probs * torch.log(assignment_probs + 1e-8), dim=(1,2))\n",
    "        loss_entropy = torch.mean(entropy)\n",
    "        \n",
    "        return loss_utility + (self.alpha_entropy * loss_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8d378a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ./models/model.pth\n",
      "Initializing Joint Training...\n",
      "Epoch 1/100 | Temp: 5.00 | Train Loss: 18.1133\n",
      "Epoch 2/100 | Temp: 4.88 | Train Loss: 18.0588\n",
      "Epoch 3/100 | Temp: 4.75 | Train Loss: 17.9592\n",
      "Epoch 4/100 | Temp: 4.63 | Train Loss: 17.8069\n",
      "Epoch 5/100 | Temp: 4.50 | Train Loss: 17.4221\n",
      "   >> Validation Loss: 48.1405 (Lower is Better)\n",
      "Epoch 6/100 | Temp: 4.38 | Train Loss: 16.4938\n",
      "Epoch 7/100 | Temp: 4.26 | Train Loss: 15.3499\n",
      "Epoch 8/100 | Temp: 4.13 | Train Loss: 14.2550\n",
      "Epoch 9/100 | Temp: 4.01 | Train Loss: 13.2727\n",
      "Epoch 10/100 | Temp: 3.89 | Train Loss: 12.3662\n",
      "   >> Validation Loss: 49.7919 (Lower is Better)\n",
      "Epoch 11/100 | Temp: 3.76 | Train Loss: 11.5117\n",
      "Epoch 12/100 | Temp: 3.64 | Train Loss: 10.7483\n",
      "Epoch 13/100 | Temp: 3.52 | Train Loss: 10.0670\n",
      "Epoch 14/100 | Temp: 3.39 | Train Loss: 9.4596\n",
      "Epoch 15/100 | Temp: 3.27 | Train Loss: 8.9824\n",
      "   >> Validation Loss: 47.1802 (Lower is Better)\n",
      "Epoch 16/100 | Temp: 3.14 | Train Loss: 8.4956\n",
      "Epoch 17/100 | Temp: 3.02 | Train Loss: 8.1796\n",
      "Epoch 18/100 | Temp: 2.90 | Train Loss: 7.8299\n",
      "Epoch 19/100 | Temp: 2.77 | Train Loss: 7.6109\n",
      "Epoch 20/100 | Temp: 2.65 | Train Loss: 7.3504\n",
      "   >> Validation Loss: 44.2370 (Lower is Better)\n",
      "Epoch 21/100 | Temp: 2.52 | Train Loss: 7.1902\n",
      "Epoch 22/100 | Temp: 2.40 | Train Loss: 7.0155\n",
      "Epoch 23/100 | Temp: 2.28 | Train Loss: 6.8602\n",
      "Epoch 24/100 | Temp: 2.15 | Train Loss: 6.6887\n",
      "Epoch 25/100 | Temp: 2.03 | Train Loss: 6.5564\n",
      "   >> Validation Loss: 41.7661 (Lower is Better)\n",
      "Epoch 26/100 | Temp: 1.91 | Train Loss: 6.3812\n",
      "Epoch 27/100 | Temp: 1.78 | Train Loss: 6.2045\n",
      "Epoch 28/100 | Temp: 1.66 | Train Loss: 6.1008\n",
      "Epoch 29/100 | Temp: 1.54 | Train Loss: 5.8659\n",
      "Epoch 30/100 | Temp: 1.41 | Train Loss: 5.7507\n",
      "   >> Validation Loss: 38.9087 (Lower is Better)\n",
      "Epoch 31/100 | Temp: 1.29 | Train Loss: 5.5921\n",
      "Epoch 32/100 | Temp: 1.16 | Train Loss: 5.3714\n",
      "Epoch 33/100 | Temp: 1.04 | Train Loss: 5.2597\n",
      "Epoch 34/100 | Temp: 0.92 | Train Loss: 5.0623\n",
      "Epoch 35/100 | Temp: 0.79 | Train Loss: 4.9783\n",
      "   >> Validation Loss: 34.1969 (Lower is Better)\n",
      "Epoch 36/100 | Temp: 0.67 | Train Loss: 4.8168\n",
      "Epoch 37/100 | Temp: 0.54 | Train Loss: 4.7173\n",
      "Epoch 38/100 | Temp: 0.42 | Train Loss: 4.9268\n",
      "Epoch 39/100 | Temp: 0.30 | Train Loss: 5.2685\n",
      "Epoch 40/100 | Temp: 0.17 | Train Loss: 7.5595\n",
      "   >> Validation Loss: 24.3951 (Lower is Better)\n",
      "Epoch 41/100 | Temp: 0.05 | Train Loss: 21.3499\n",
      "Epoch 42/100 | Temp: 0.05 | Train Loss: 19.0387\n",
      "Epoch 43/100 | Temp: 0.05 | Train Loss: 16.3217\n",
      "Epoch 44/100 | Temp: 0.05 | Train Loss: 13.4875\n",
      "Epoch 45/100 | Temp: 0.05 | Train Loss: 11.2132\n",
      "   >> Validation Loss: 10.8178 (Lower is Better)\n",
      "Epoch 46/100 | Temp: 0.05 | Train Loss: 8.8916\n",
      "Epoch 47/100 | Temp: 0.05 | Train Loss: 6.8961\n",
      "Epoch 48/100 | Temp: 0.05 | Train Loss: 5.3398\n",
      "Epoch 49/100 | Temp: 0.05 | Train Loss: 4.1642\n",
      "Epoch 50/100 | Temp: 0.05 | Train Loss: 3.6821\n",
      "   >> Validation Loss: 3.9758 (Lower is Better)\n",
      "Epoch 51/100 | Temp: 0.05 | Train Loss: 3.3826\n",
      "Epoch 52/100 | Temp: 0.05 | Train Loss: 3.2997\n",
      "Epoch 53/100 | Temp: 0.05 | Train Loss: 3.1665\n",
      "Epoch 54/100 | Temp: 0.05 | Train Loss: 2.9806\n",
      "Epoch 55/100 | Temp: 0.05 | Train Loss: 2.8962\n",
      "   >> Validation Loss: 3.1578 (Lower is Better)\n",
      "Epoch 56/100 | Temp: 0.05 | Train Loss: 2.9066\n",
      "Epoch 57/100 | Temp: 0.05 | Train Loss: 2.8161\n",
      "Epoch 58/100 | Temp: 0.05 | Train Loss: 2.7950\n",
      "Epoch 59/100 | Temp: 0.05 | Train Loss: 2.6973\n",
      "Epoch 60/100 | Temp: 0.05 | Train Loss: 2.6437\n",
      "   >> Validation Loss: 2.8112 (Lower is Better)\n",
      "Epoch 61/100 | Temp: 0.05 | Train Loss: 2.6281\n",
      "Epoch 62/100 | Temp: 0.05 | Train Loss: 2.6759\n",
      "Epoch 63/100 | Temp: 0.05 | Train Loss: 2.5294\n",
      "Epoch 64/100 | Temp: 0.05 | Train Loss: 2.5011\n",
      "Epoch 65/100 | Temp: 0.05 | Train Loss: 2.5297\n",
      "   >> Validation Loss: 2.5734 (Lower is Better)\n",
      "Epoch 66/100 | Temp: 0.05 | Train Loss: 2.4954\n",
      "Epoch 67/100 | Temp: 0.05 | Train Loss: 2.4311\n",
      "Epoch 68/100 | Temp: 0.05 | Train Loss: 2.4167\n",
      "Epoch 69/100 | Temp: 0.05 | Train Loss: 2.3826\n",
      "Epoch 70/100 | Temp: 0.05 | Train Loss: 2.4363\n",
      "   >> Validation Loss: 2.4137 (Lower is Better)\n",
      "Epoch 71/100 | Temp: 0.05 | Train Loss: 2.3220\n",
      "Epoch 72/100 | Temp: 0.05 | Train Loss: 2.3784\n",
      "Epoch 73/100 | Temp: 0.05 | Train Loss: 2.3017\n",
      "Epoch 74/100 | Temp: 0.05 | Train Loss: 2.2510\n",
      "Epoch 75/100 | Temp: 0.05 | Train Loss: 2.2200\n",
      "   >> Validation Loss: 2.2743 (Lower is Better)\n",
      "Epoch 76/100 | Temp: 0.05 | Train Loss: 2.1797\n",
      "Epoch 77/100 | Temp: 0.05 | Train Loss: 2.2290\n",
      "Epoch 78/100 | Temp: 0.05 | Train Loss: 2.1547\n",
      "Epoch 79/100 | Temp: 0.05 | Train Loss: 2.2062\n",
      "Epoch 80/100 | Temp: 0.05 | Train Loss: 2.1321\n",
      "   >> Validation Loss: 2.1672 (Lower is Better)\n",
      "Epoch 81/100 | Temp: 0.05 | Train Loss: 2.1256\n",
      "Epoch 82/100 | Temp: 0.05 | Train Loss: 2.1506\n",
      "Epoch 83/100 | Temp: 0.05 | Train Loss: 2.1076\n",
      "Epoch 84/100 | Temp: 0.05 | Train Loss: 1.9530\n",
      "Epoch 85/100 | Temp: 0.05 | Train Loss: 2.0831\n",
      "   >> Validation Loss: 2.0420 (Lower is Better)\n",
      "Epoch 86/100 | Temp: 0.05 | Train Loss: 2.0729\n",
      "Epoch 87/100 | Temp: 0.05 | Train Loss: 2.0073\n",
      "Epoch 88/100 | Temp: 0.05 | Train Loss: 1.9100\n",
      "Epoch 89/100 | Temp: 0.05 | Train Loss: 1.9666\n",
      "Epoch 90/100 | Temp: 0.05 | Train Loss: 1.8778\n",
      "   >> Validation Loss: 1.9301 (Lower is Better)\n",
      "Epoch 91/100 | Temp: 0.05 | Train Loss: 1.8590\n",
      "Epoch 92/100 | Temp: 0.05 | Train Loss: 1.9270\n",
      "Epoch 93/100 | Temp: 0.05 | Train Loss: 1.8744\n",
      "Epoch 94/100 | Temp: 0.05 | Train Loss: 1.7984\n",
      "Epoch 95/100 | Temp: 0.05 | Train Loss: 1.8466\n",
      "   >> Validation Loss: 1.8230 (Lower is Better)\n",
      "Epoch 96/100 | Temp: 0.05 | Train Loss: 1.8075\n",
      "Epoch 97/100 | Temp: 0.05 | Train Loss: 1.7779\n",
      "Epoch 98/100 | Temp: 0.05 | Train Loss: 1.7734\n",
      "Epoch 99/100 | Temp: 0.05 | Train Loss: 1.7356\n",
      "Epoch 100/100 | Temp: 0.05 | Train Loss: 1.6977\n",
      "   >> Validation Loss: 1.6830 (Lower is Better)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Annealing Schedule \n",
    "START_TEMP = 5.0   # High temp = Fuzzy exploration\n",
    "END_TEMP = 0.05     # Low temp = Hard decision (Deployment mode)\n",
    "ANNEAL_EPOCHS = 40 # Reach end_temp by epoch 20\n",
    "\n",
    "def get_current_temperature(epoch):\n",
    "    \"\"\"Calculates Softmax temperature for the current epoch.\"\"\"\n",
    "    if epoch >= ANNEAL_EPOCHS:\n",
    "        return END_TEMP\n",
    "    decay = (START_TEMP - END_TEMP) / ANNEAL_EPOCHS\n",
    "    return START_TEMP - (decay * epoch)\n",
    "\n",
    "def compute_jains_fairness(user_rates):\n",
    "    \"\"\"Auxiliary metric to track fairness progress.\"\"\"\n",
    "    # Jain's Index = (Sum R)^2 / (K * Sum R^2)\n",
    "    K = user_rates.shape[1]\n",
    "    sum_r = torch.sum(user_rates, dim=1)\n",
    "    sum_r_sq = torch.sum(user_rates**2, dim=1)\n",
    "    return torch.mean((sum_r**2) / (K * sum_r_sq + 1e-8))\n",
    "\n",
    "# --- Validation Function ---\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_rate = 0.0\n",
    "    val_fairness = 0.0\n",
    "    \n",
    "    # Use hard/sharp temperature for validation to mimic real performance\n",
    "    val_temp = END_TEMP \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            channels_raw = batch[0].to(device)\n",
    "            \n",
    "            # --- AI World: Normalization ---\n",
    "            # Even in validation, we must normalize inputs same as training\n",
    "            user_pwr = torch.mean(torch.abs(channels_raw)**2, dim=(2,3), keepdim=True)\n",
    "            user_scale = torch.sqrt(user_pwr + 1e-12)\n",
    "            channels_input = channels_raw / user_scale\n",
    "            \n",
    "            # Forward\n",
    "            probs, powers = model(channels_input, temperature=val_temp)\n",
    "            \n",
    "            # --- Physics World: Real Metrics ---\n",
    "            # We calculate loss using RAW channels\n",
    "            loss = criterion(probs, powers, channels_raw)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate actual metrics for logging\n",
    "            # Re-use logic from loss function to extract User Rates\n",
    "            # (Simplified here for display)\n",
    "            # A good proxy is looking at the negative utility part of the loss\n",
    "            # But let's verify Sum Rate properly if possible.\n",
    "            # For now, we trust the Loss represents -Utility.\n",
    "            \n",
    "    avg_loss = val_loss / len(val_loader)\n",
    "    print(f\"   >> Validation Loss: {avg_loss:.4f} (Lower is Better)\")\n",
    "    return avg_loss\n",
    "\n",
    "# --- Main Training Loop ---\n",
    "def train_joint_scheduler(model, train_loader, val_loader):\n",
    "    print(\"Initializing Joint Training...\")\n",
    "    \n",
    "    # Optimizer: Updates both LWM backbone (if unfrozen) and Heads [cite: 20]\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Loss: High-Fidelity Joint Utility [cite: 22]\n",
    "    # noise_var should match your 'Edge User' calibrated noise (e.g., 4e-12)\n",
    "    criterion = JointUtilityLoss(alpha_entropy=0.05, alpha_power=0.01, noise_var=noise_variance)\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        current_temp = get_current_temperature(epoch)\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            channels_raw = batch[0].to(DEVICE)\n",
    "            \n",
    "            # --- 1. THE \"TWO WORLDS\" SPLIT ---\n",
    "            \n",
    "            # WORLD A: The AI Input (Relative Context)\n",
    "            # Normalize so LWM sees \"How good is this block for THIS user?\"\n",
    "            user_avg_pwr = torch.mean(torch.abs(channels_raw)**2, dim=(2,3), keepdim=True)\n",
    "            user_scale = torch.sqrt(user_avg_pwr + 1e-12)\n",
    "            channels_input = channels_raw / user_scale\n",
    "            \n",
    "            # WORLD B: The Physics Loss (Absolute Watts)\n",
    "            # We keep 'channels_raw' to calculate real SINR and Capacity\n",
    "            \n",
    "            # --- 2. Forward Pass ---\n",
    "            # Feed NORMALIZED channels to model\n",
    "            # Pass temperature for annealing \n",
    "            probs, powers = model(channels_input, temperature=current_temp)\n",
    "            \n",
    "            # --- 3. Loss Calculation ---\n",
    "            # Feed RAW channels to loss to get accurate SINR\n",
    "            loss = criterion(probs, powers, channels_raw)\n",
    "            \n",
    "            # --- 4. Optimization ---\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient Clipping (Crucial for LWM stability)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        # --- Logging & Validation ---\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Temp: {current_temp:.2f} | Train Loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validate every 5 epochs or at the end\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            validate(model, val_loader, criterion, DEVICE)\n",
    "\n",
    "# --- Usage ---\n",
    "from thesis.dataset import Tokenizer\n",
    "from thesis.downstream_models import CarrierAllocation\n",
    "from thesis.lwm_model import lwm\n",
    "\n",
    "tokenizer = Tokenizer(4, 4)\n",
    "lwm_model = lwm.from_pretrained(\"./models/model.pth\")\n",
    "assignment_head = AssignmentHead()\n",
    "allocation_head = PowerAllocationHead()\n",
    "full_model = CarrierAllocation(tokenizer, lwm_model, assignment_head, allocation_head)\n",
    "\n",
    "train_joint_scheduler(full_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f94bd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Benchmark (Noise=9.4e-10)...\n",
      "\n",
      "================================================================================\n",
      "FINAL BENCHMARK RESULTS\n",
      "================================================================================\n",
      "              Algorithm  Sum Rate (bps/Hz)  Fairness (Jain's)  Edge User Rate\n",
      "           Greedy (EPA)          54.437279           0.957083        5.247259\n",
      "Proportional Fair (EPA)          29.393646           0.764104        0.549059\n",
      "      Round Robin (EPA)          29.164457           0.734636        0.239863\n",
      "       AI Model (Joint)          27.190149           0.733292        0.327540\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class WirelessBenchmark:\n",
    "    def __init__(self, model, patch_cols, noise_variance=1e-11, total_power=1.0, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.patch_cols = patch_cols\n",
    "        self.noise = noise_variance\n",
    "        self.total_power = total_power\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "\n",
    "        # Storage for global stats (to fix the \"Mean of Percentiles\" error)\n",
    "        self.global_results = {\n",
    "            \"Greedy (EPA)\": [], \"Round Robin (EPA)\": [], \n",
    "            \"Proportional Fair (EPA)\": [], \"AI Model (Joint)\": []\n",
    "        }\n",
    "\n",
    "    def _calculate_high_fidelity_rates(self, channels, allocation_map, power_map_per_block):\n",
    "        \"\"\"\n",
    "        Helper: Returns the raw rates per user (B, K) given an allocation.\n",
    "        Used to decouple 'Calculation' from 'Aggregation'.\n",
    "        \"\"\"\n",
    "        B, K, M, SC = channels.shape\n",
    "        Num_Blocks = allocation_map.shape[1]\n",
    "        SC_per_block = SC // Num_Blocks\n",
    "\n",
    "        # 1. Expand Block Decisions to Subcarriers\n",
    "        # (Batch, Num_Blocks) -> (Batch, SC)\n",
    "        alloc_sc = allocation_map.repeat_interleave(SC_per_block, dim=1)\n",
    "        \n",
    "        # Power per subcarrier = Block_Power / Subcarriers_in_Block\n",
    "        power_per_sc_val = power_map_per_block / SC_per_block\n",
    "        power_sc = power_per_sc_val.repeat_interleave(SC_per_block, dim=1)\n",
    "        \n",
    "        # 2. Physics: Calculate SNR per Subcarrier (MRC)\n",
    "        # (Batch, Users, SC)\n",
    "        channel_gains_sc = torch.sum(torch.abs(channels)**2, dim=2)\n",
    "        \n",
    "        # Gather winner gains\n",
    "        winner_gains_sc = torch.gather(channel_gains_sc, 1, alloc_sc.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # 3. Calculate Rate\n",
    "        snr_sc = (power_sc * winner_gains_sc) / self.noise\n",
    "        rate_sc = torch.log2(1 + snr_sc)\n",
    "        \n",
    "        # 4. Sum Rates per User\n",
    "        user_indices = torch.arange(K, device=self.device).view(1, K, 1)\n",
    "        mask = (alloc_sc.unsqueeze(1) == user_indices).float()\n",
    "        \n",
    "        # Result: (Batch, K) - Total Rate for each user in the batch\n",
    "        user_rates = torch.sum(rate_sc.unsqueeze(1) * mask, dim=2)\n",
    "        \n",
    "        return user_rates\n",
    "\n",
    "    def run_baselines(self, channels):\n",
    "        B, K, M, SC = channels.shape\n",
    "        num_blocks = SC // self.patch_cols\n",
    "        \n",
    "        # --- Pre-calculate Block Metrics for Decisions ---\n",
    "        # 1. Block Gains (Averaged Fading)\n",
    "        mag_sq = torch.abs(channels)**2\n",
    "        gain_freq = torch.mean(mag_sq, dim=2)\n",
    "        gain_reshaped = gain_freq.view(B, K, num_blocks, self.patch_cols)\n",
    "        block_gains = torch.mean(gain_reshaped, dim=3)\n",
    "        \n",
    "        # 2. EPA Power\n",
    "        power_per_block = self.total_power / num_blocks\n",
    "        epa_map = torch.full((B, num_blocks), power_per_block, device=self.device)\n",
    "        \n",
    "        # 3. Decision SNR (Linear)\n",
    "        block_snrs_est = (block_gains * power_per_block) / self.noise\n",
    "        \n",
    "        # 4. Decision Rates (Logarithmic) - CRITICAL FOR PF\n",
    "        block_rates_est = torch.log2(1 + block_snrs_est)\n",
    "\n",
    "        # --- Algorithm 1: Greedy ---\n",
    "        # Maximize Rate on each block\n",
    "        greedy_alloc = torch.argmax(block_rates_est, dim=1)\n",
    "        rates_greedy = self._calculate_high_fidelity_rates(channels, greedy_alloc, epa_map)\n",
    "        self.global_results[\"Greedy (EPA)\"].append(rates_greedy)\n",
    "\n",
    "        # --- Algorithm 2: Round Robin (Randomized Start) ---\n",
    "        # Fix: Randomize start index to prevent starvation of users K > N\n",
    "        start_offsets = torch.randint(0, K, (B, 1), device=self.device)\n",
    "        block_indices = torch.arange(num_blocks, device=self.device).unsqueeze(0) # (1, Blocks)\n",
    "        rr_alloc = (block_indices + start_offsets) % K\n",
    "        \n",
    "        rates_rr = self._calculate_high_fidelity_rates(channels, rr_alloc, epa_map)\n",
    "        self.global_results[\"Round Robin (EPA)\"].append(rates_rr)\n",
    "\n",
    "        # --- Algorithm 3: Proportional Fair (Corrected) ---\n",
    "        # Metric: Rate_est / Avg_Rate_est\n",
    "        # We approximate Avg_Rate_est as the mean rate available to the user across all blocks in this snapshot\n",
    "        user_avg_rate = torch.mean(block_rates_est, dim=2, keepdim=True)\n",
    "        pf_metric = block_rates_est / (user_avg_rate + 1e-8)\n",
    "        \n",
    "        pf_alloc = torch.argmax(pf_metric, dim=1)\n",
    "        rates_pf = self._calculate_high_fidelity_rates(channels, pf_alloc, epa_map)\n",
    "        self.global_results[\"Proportional Fair (EPA)\"].append(rates_pf)\n",
    "\n",
    "    def run_ai(self, channels):\n",
    "        # Normalize\n",
    "        user_pwr = torch.mean(torch.abs(channels)**2, dim=(2,3), keepdim=True)\n",
    "        user_scale = torch.sqrt(user_pwr + 1e-12)\n",
    "        channels_norm = channels / user_scale\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            probs, powers = self.model(channels_norm, temperature=0.01)\n",
    "            \n",
    "            # Assignment\n",
    "            ai_alloc = torch.argmax(probs, dim=1)\n",
    "            \n",
    "            # Power Extraction & Re-normalization\n",
    "            ai_power_raw = torch.gather(powers, 1, ai_alloc.unsqueeze(1)).squeeze(1)\n",
    "            winner_sum = torch.sum(ai_power_raw, dim=1, keepdim=True)\n",
    "            scale = self.total_power / (winner_sum + 1e-12)\n",
    "            ai_power_map = ai_power_raw * scale\n",
    "            \n",
    "        rates_ai = self._calculate_high_fidelity_rates(channels, ai_alloc, ai_power_map)\n",
    "        self.global_results[\"AI Model (Joint)\"].append(rates_ai)\n",
    "\n",
    "    def get_summary(self):\n",
    "        \"\"\"Computes global statistics across the entire dataset.\"\"\"\n",
    "        summary_data = []\n",
    "        \n",
    "        for alg_name, rate_list in self.global_results.items():\n",
    "            if not rate_list: continue\n",
    "            \n",
    "            # Concatenate all batches -> (Total_Samples, K)\n",
    "            all_rates = torch.cat(rate_list, dim=0)\n",
    "            \n",
    "            # Mask Ghosts globally\n",
    "            # We assume a user is \"active\" if they have non-zero rate potential in general\n",
    "            # For simplicity, we filter 0.0 rates if they are strictly 0 due to channel \n",
    "            # But safer: Filter based on rate > 1e-6 (Effective Zero)\n",
    "            active_rates = all_rates[all_rates > 1e-6] \n",
    "            \n",
    "            # 1. Sum Rate (Avg per cell)\n",
    "            # Sum over users, then Mean over samples\n",
    "            avg_sum_rate = torch.mean(torch.sum(all_rates, dim=1)).item()\n",
    "            \n",
    "            # 2. Fairness (Global Jain's)\n",
    "            # Calculated per sample, then averaged\n",
    "            sum_r = torch.sum(all_rates, dim=1)\n",
    "            sum_r_sq = torch.sum(all_rates**2, dim=1)\n",
    "            # Count active users per sample (users with Rate > 0)\n",
    "            n_active = torch.sum(all_rates > 1e-6, dim=1)\n",
    "            \n",
    "            # Avoid div/0 for empty samples\n",
    "            valid_mask = n_active > 0\n",
    "            jain_samples = (sum_r[valid_mask]**2) / (n_active[valid_mask] * sum_r_sq[valid_mask] + 1e-12)\n",
    "            avg_fairness = torch.mean(jain_samples).item()\n",
    "            \n",
    "            # 3. Edge Rate (True Global 5th Percentile)\n",
    "            edge_rate = torch.quantile(active_rates, 0.05).item()\n",
    "            \n",
    "            summary_data.append({\n",
    "                \"Algorithm\": alg_name,\n",
    "                \"Sum Rate (bps/Hz)\": avg_sum_rate,\n",
    "                \"Fairness (Jain's)\": avg_fairness,\n",
    "                \"Edge User Rate\": edge_rate\n",
    "            })\n",
    "            \n",
    "        df = pd.DataFrame(summary_data)\n",
    "        return df.sort_values(by=\"Sum Rate (bps/Hz)\", ascending=False)\n",
    "\n",
    "def run_benchmark(model, test_loader, patch_cols, noise_var, device):\n",
    "    print(f\"Running Benchmark (Noise={noise_var:.1e})...\")\n",
    "    benchmarker = WirelessBenchmark(model, patch_cols, noise_variance=noise_var, device=device)\n",
    "    \n",
    "    for batch in test_loader:\n",
    "        channels = batch[0].to(device)\n",
    "        benchmarker.run_baselines(channels)\n",
    "        benchmarker.run_ai(channels)\n",
    "        \n",
    "    summary = benchmarker.get_summary()\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL BENCHMARK RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary.to_string(index=False))\n",
    "\n",
    "run_benchmark(full_model, test_loader, 4, noise_variance, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e02aaf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_allocation(model, loader, patch_cols, device):\n",
    "    model.eval()\n",
    "    \n",
    "    # Get one batch\n",
    "    batch = next(iter(loader))\n",
    "    channels = batch[0].to(device)\n",
    "    \n",
    "    # 1. Run AI\n",
    "    # Normalize\n",
    "    user_pwr = torch.mean(torch.abs(channels)**2, dim=(2,3), keepdim=True)\n",
    "    user_scale = torch.sqrt(user_pwr + 1e-12)\n",
    "    channels_norm = channels / user_scale\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probs, powers = model(channels_norm, temperature=0.01)\n",
    "        # Allocation Indices (Batch, Blocks)\n",
    "        alloc_idx = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "    \n",
    "    # 2. Plot for Sample 0\n",
    "    sample_idx = 0\n",
    "    allocation = alloc_idx[sample_idx] # Shape (Blocks,)\n",
    "    \n",
    "    # Create a heatmap grid\n",
    "    # We want to see which User (Y-axis) gets which Block (X-axis)\n",
    "    num_users = probs.shape[1]\n",
    "    num_blocks = probs.shape[2]\n",
    "    \n",
    "    grid = np.zeros((num_users, num_blocks))\n",
    "    \n",
    "    for b in range(num_blocks):\n",
    "        u = allocation[b]\n",
    "        grid[u, b] = 1.0 # User u owns block b\n",
    "        \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.heatmap(grid, cmap=\"Blues\", cbar=False, linewidths=0.1, linecolor='gray')\n",
    "    plt.title(f\"AI Resource Allocation Map (Sample {sample_idx})\")\n",
    "    plt.xlabel(\"Frequency Blocks\")\n",
    "    plt.ylabel(\"User Index\")\n",
    "    plt.show()\n",
    "\n",
    "# Run it\n",
    "visualize_allocation(full_model, test_loader, 4, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38458cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
